{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Overview\n",
    "#   From the beginning, since the first printed newspaper, every news that makes into a page has had a specific section allotted to it. Although pretty much everything changed in newspapers from the ink to the type of paper used, this proper categorization of news was carried over by generations and even to the digital versions of the newspaper. Newspaper articles are not limited to a few topics or subjects, it covers a wide range of interests from politics to sports to movies and so on. For long, this process of sectioning was done manually by people but now technology can do it without much effort. In this hackathon, Data Science and Machine Learning enthusiasts like you will use Natural Language Processing to predict which genre or category a piece of news will fall in to from the story. Size of training set: 7,628 records Size of test set: 2,748 records FEATURES: STORY:  A part of the main content of the article to be published as a piece of news. SECTION: The genre/category the STORY falls in. There are four distinct sections where each story may fall in to. \n",
    "#   **The Sections are labelled as follows : **\n",
    "# *       Politics: 0 \n",
    "# *       Technology: 1 \n",
    "# *       Entertainment: 2 \n",
    "# *       Business: 3\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "get_ipython().system('pip install -U texthero')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import texthero as hero\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import matplotlib as mpl\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "train = pd.read_excel('/kaggle/input/new-category/Data_Train.xlsx')\n",
    "test= pd.read_excel('/kaggle/input/new-category/Data_Test.xlsx')\n",
    "\n",
    "display(train.sample(5))\n",
    "display(train.info())\n",
    "display(test.info())\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "train.STORY[0]\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "train.SECTION.value_counts(normalize=True)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "combined_df = pd.concat([train.drop('SECTION',axis=1),test])\n",
    "combined_df.info()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "hero.visualization.wordcloud(combined_df['STORY'], max_words=1000,background_color='BLACK')\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "hero.Word2Vec\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "combined_df['cleaned_text']=(combined_df['STORY'].pipe(hero.remove_angle_brackets)\n",
    "                    .pipe(hero.remove_brackets)\n",
    "                    .pipe(hero.remove_curly_brackets)\n",
    "                    .pipe(hero.remove_diacritics)\n",
    "                    .pipe(hero.remove_digits)\n",
    "                    .pipe(hero.remove_html_tags)\n",
    "                    .pipe(hero.remove_punctuation)\n",
    "                    .pipe(hero.remove_round_brackets)\n",
    "                    .pipe(hero.remove_square_brackets)\n",
    "                    .pipe(hero.remove_stopwords)\n",
    "                    .pipe(hero.remove_urls)\n",
    "                    .pipe(hero.remove_whitespace)\n",
    "                    .pipe(hero.lowercase))\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def word_lemma(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemma = [lemm.lemmatize(word) for word in words]\n",
    "    joined_text = \" \".join(lemma)\n",
    "    return joined_text\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "combined_df['lemmatized_text'] = combined_df.cleaned_text.apply(lambda x: word_lemma(x))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "text = []\n",
    "for i in range(len(combined_df)):\n",
    "    review = nltk.word_tokenize(combined_df['lemmatized_text'].iloc[i])\n",
    "    review = ' '.join(review)\n",
    "    text.append(review)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,f1_score,plot_confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "X = combined_df['lemmatized_text'].iloc[:7628]\n",
    "test_df = combined_df['lemmatized_text'].iloc[7628:]\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "cv = CountVectorizer(max_features=9000)\n",
    "cv.fit(X)\n",
    "X = cv.transform(X)\n",
    "test_df = cv.transform(test_df)\n",
    "\n",
    "y = train.SECTION\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "tfid = TfidfVectorizer(max_features=9000)\n",
    "tfid.fit(X)\n",
    "X = tfid.transform(X)\n",
    "test_df = tfid.transform(test_df)\n",
    "\n",
    "y = train.SECTION\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_new,y_new = smote.fit_resample(X,y)\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42,stratify=y_new)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xg=XGBClassifier()\n",
    "xg.fit(X_train,y_train)\n",
    "y_pred = xg.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "cat=CatBoostClassifier(task_type='GPU')\n",
    "cat.fit(X_train,y_train)\n",
    "y_pred = cat.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "predictions=xg.predict(test_df)\n",
    "submissions = pd.DataFrame({'SECTION':predictions})\n",
    "submissions.to_csv('./sub8.csv',index=False,header=True)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
