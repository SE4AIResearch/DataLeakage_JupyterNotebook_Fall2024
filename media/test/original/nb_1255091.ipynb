{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Informal settlements of Syrian refugees in Lebanon\n",
    "# \n",
    "# ## Overview\n",
    "# * Data was downloaded from [The Humanitarian Data Exchange](https://data.humdata.org/dataset/informal-settlements-refugees-living-in-informal-settlements)\n",
    "# * Description from the above link: \n",
    "# > In Lebanon Syrian refugees are scattered all over the country, mostly living in the cities and villages with host communities renting or occupying abandoned or unfinished buildings, however over 30% of the refugees are living in tents in Informal Settlements. Informal Settlements are collections of tents (not more than 20 tents).\n",
    "# \n",
    "# ## Aims of this project\n",
    "# * Learn how to plot maps, and visualize data on maps\n",
    "# * Practice exploration of variables and feature cleaning \n",
    "# * Practice machine learning and more in-depth cross-validation\n",
    "# * packages: pandas, numpy, matplotlib, mpl_toolkits.basemap, scikit-learn\n",
    "# \n",
    "# ## Summary\n",
    "# * Explored the variables in the dataset to investigate correlations with the Status of settlements. \n",
    "# * Cleaned up data by removing NANs and irrelevant variables.\n",
    "# * Visualized a heatmap of settlement locations on the country map using basemap.\n",
    "# * Used Google Geocoding API to find latitude and longitude data for specific Governorates in Lebanon, then plot their locations on the map.\n",
    "# * Mapped categorical variables into ordinals, and created new ordinal categories out of numerical variables.\n",
    "# * Used random forest machine learning from scikit-learn to predict the status of settlements.\n",
    "# * Ran cross-validations, and visualized ROC curves.\n",
    "# * Explored feature importance.\n",
    "# ___\n",
    "\n",
    "# ## Variable definitions\n",
    "# * **PCode**: Officially assigned PCode. This is unique to every IS. It holds the CAS (Central Administration of Statistics) Number, the IS code and the sequemtial number of the IS. Use this code to refer to the IS.  Medair assigns PCodes. This code should be used as a reference to any documentation citing an informal settlement.\n",
    "# * **PCode Name**: P-Code based naming of the informal settlement, based on the name of the cadastral and and the site's 3 digit sequential number. This is the official name of the settlement.\n",
    "# * **Governorate**: Governorate\n",
    "# * **District**: District\n",
    "# * **Cadastral Name**: Name of the Cadaster where the settlement is located\n",
    "# * **Local Name**: Local Name of the Settlement according to NGOs working there or residents\n",
    "# * **Latitude**: Latitude\n",
    "# * **Longitude**: Longitude\n",
    "# * **Shelter Type**: Informal settlement\n",
    "# * **Status**: Active, Less than 4,Inactive, Not Willing or Erroneous\n",
    "# * **Number of Tents**: Number of tents in the settlements verified by physical observation\n",
    "# * **Number of Individuals**: Number of individuals living in Tents, verified by asking each tent's residents how many people sleep there each night.\n",
    "# * **Date of the current update**: What date the update was taken - accuracy of the data is only for this particular date as settlements change frequently.\n",
    "# * **Updated By**: Partner who undertook the update, 'Other' if from a secondary data source and not yet verified\n",
    "# * **Updated On**: \"Sweep\" in which the site was updated\n",
    "# * **Discovery Date**: Calculated by running a query on all IAMP data from IAMP 1 to current IAMP to determine the first available date of update\n",
    "# * **Date the site was created**: Date the settlement was first established\n",
    "# * **Notes**: Any significant findings or explanation of the data\n",
    "# \t\n",
    "# \t\n",
    "# * **Latrines**: Number of Latrines in the settlement.Â  This should be all free standing functional latrines, not just those built by NGOs, verified by physical observations.\n",
    "# * **Water Capacity in L**: The water storage capacity availble on the site in liters. Liters of water tanks are counted and  verified by physical observations.\n",
    "# * **Type of Water Source**: What is the primary source of water for the settlement? Verified by asking the Shawish.\n",
    "# * **Waste Disposal**: What is the primary method of waste disposal for the settlement? Verified by asking the Shawish.\n",
    "# * **Type of Contract**: Type of Contract between the Landlord and the refugees (Verbal, Written or None). Verified by asking the Shawish.\n",
    "# * **Waste Water Disposal**: What is the primary method of waste water disposal for the settlement? Verified by asking the Shawish.\n",
    "# * **Type of Internet Connection**: What kind of internet connection is used by the IS residents. Verified by asking the Shawish.\n",
    "# * **Consultation Fee for PHC (3000/5000)**: If they know they should pay between 3000 and 5000 Lebanese Pounds for each consultation at primary health care centre. Verified by asking the Shawish.\n",
    "# * **Free Vaccination for Children under 12**: If they know that refugee children < 12 have free access to vaccination at Ministry of Health facilities. Verified by asking the Shawish.\n",
    "# * **Number of SSBs**: Number of SSBs in the direct confines of the informal settlement. Verified by physical observation\n",
    "# * **Number of Ind in SSBs**: Number of individuals living in SSBs, verified by asking each SSB's residents how many people sleep there each night.\n",
    "\n",
    "# ## Question to ask\n",
    "# Settlements have a **Status** of *Active*, *Inactive* or *Less than 4* meaning less than 4 tents exist in that location. The latter category is on the verge of becoming *Inactive*, i.e., refugees leaving the settlement, moving elsewhere, etc. \n",
    "# ### Can we predict if a settlement is on the verge of becoming *Inactive*, i.e., if its **Status** is *Less than 4* or *Active*, based on other information in the dataset?\n",
    "# ___\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "from matplotlib.ticker import LogFormatter \n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib import cm\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# acquire data\n",
    "df = pd.read_csv('ListofInformalSettlements_29_DEC_2016_All_sites.csv')\n",
    "df_base = df\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "df_base.head()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "df['Governorate'].unique()\n",
    "\n",
    "\n",
    "# ## Missing governorates\n",
    "# * Akkar Governorate is missing, and has been lumped with the North Governorate\n",
    "# * Baalbak-Hermel Governorate is missing, and has been lumped with the Bekaa Governorate\n",
    "# * The reason may be that the two missing governorates were only added in 2003, see http://www.localiban.org/article5075.html\n",
    "\n",
    "# ## Heatmap of refugee informal settlement locations\n",
    "# * Below we can see that the highest concentration of informal settlements is in the North and East of Lebanon. This is possibly because Syrian borders are located at the North and East. \n",
    "# * No real concentration in the capital of Beirut. Could be due to the high cost of living there and a more controlled refugee management (if any).\n",
    "# \n",
    "\n",
    "# In[87]:\n",
    "\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error, json, time\n",
    "\n",
    "# initialize dictionary\n",
    "LocDict = dict()\n",
    "\n",
    "# get Google Geocoding API\n",
    "api_table = pd.read_csv('../../PythonLearningSideProjects/APIs/Sarine_APIs.csv')\n",
    "google_api = api_table['API key'][api_table['Name'].isin([\"google_api\"])].values[0]\n",
    "\n",
    "# retrieve latitude and longitude for each of the Governorates and store them in a dictionary\n",
    "labeltext = df_base['Governorate'].unique()\n",
    "for ii in range(len(labeltext)):\n",
    "    address = r\"\" + labeltext[ii]+\" Governorate, Lebanon\"\n",
    "    addP = \"address=\" + address.replace(\" \",\"+\")\n",
    "    GeoUrl = r\"https://maps.googleapis.com/maps/api/geocode/json?\" + addP + \"&key=\" + google_api\n",
    "    response = urllib.request.urlopen(GeoUrl)\n",
    "    jsonRaw = response.read()\n",
    "    jsonData = json.loads(jsonRaw)\n",
    "    if jsonData['status'] == 'OK':\n",
    "        res = jsonData['results'][0]\n",
    "        LocDict[labeltext[ii]] = [res['geometry']['location']['lng'],res['geometry']['location']['lat']]     \n",
    "    else:\n",
    "        LocDict = {None,None,None} # this line has not been verified\n",
    "    \n",
    "print(LocDict)\n",
    "\n",
    "\n",
    "# In[88]:\n",
    "\n",
    "\n",
    "uppercorner = [34.7, 36.8] #lat, long of map upper corner\n",
    "lowercorner = [33.018977, 34.9] #lat, long of map lower corner\n",
    "\n",
    "xytexts=[(-50,5),(-120,0),(-50,5),(-50,5),(-50,5),(-50,10)] # text offsets\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111)\n",
    "m = Basemap(projection = 'merc',llcrnrlon=lowercorner[1],llcrnrlat=lowercorner[0],urcrnrlon=uppercorner[1],urcrnrlat=uppercorner[0], lat_ts=lowercorner[0], resolution='i',epsg=22770)\n",
    "#http://server.arcgisonline.com/arcgis/rest/services ESRI_Imagery_World_2D ESRI_StreetMap_World_2D NatGeo_World_Map\n",
    "\n",
    "m.arcgisimage(service='ESRI_Imagery_World_2D', xpixels = 300, verbose= True)\n",
    "#m.drawcoastlines(linewidth=2)\n",
    "#m.drawcountries(linewidth=2,zorder=1)\n",
    "m.drawrivers(color='darkcyan',linewidth=1.5,zorder=1)\n",
    "\n",
    "plotvar = df['Number of Ind'].values+df['Number of Ind in SSBs'].values\n",
    "#plotvar = df['Water Capacity in L'].values\n",
    "x, y = m(df['Longitude'].values, df['Latitude'].values)\n",
    "\n",
    "for ii in range(len(labeltext)):\n",
    "    xlab,ylab = m(labelcoord[ii][0], labelcoord[ii][1])\n",
    "    xlab,ylab = m(LocDict[labeltext[ii]][0], LocDict[labeltext[ii]][1])\n",
    "    m.plot(xlab, ylab, color = 'lightgray', marker = '*', markersize=15,zorder=2)\n",
    "    plt.annotate(labeltext[ii],xy=(xlab,ylab),xytext=xytexts[ii],textcoords='offset points',fontsize=16,color='lightgray')\n",
    "\n",
    "# plot a scatter of the concentration of settlements in log scale\n",
    "m.hexbin(x, y,  gridsize=100, bins = 'log', mincnt=1, cmap=cm.YlOrRd_r,zorder=2); # bins='log' is log colorscale and yellow is highest (http://matplotlib.org/1.2.1/examples/pylab_examples/hexbin_demo.html)\n",
    "formatter = LogFormatter(10, labelOnlyBase=False) #,norm=matplotlib.colors.LogNorm()\n",
    "cb = plt.colorbar() #format=formatter\n",
    "cb.set_label('Number of  settlements [log10(N)]')\n",
    "m.readshapefile('/Users/Sarine/Documents/Sarine/Other/PythonLearningSideProjects/lebanon-refugee-data/LBN_adm/LBN_adm1', 'areas',linewidth = 2,zorder = 1)\n",
    "#shapefile data for Lebanon downloaded from: http://www.diva-gis.org/gdata\n",
    "#m.areas_info contains information about each of the areas defined in shapefile\n",
    "\n",
    "# #only plot the 6 governorates in magenta: \n",
    "# for info, shape in zip(m.areas_info, m.areas):\n",
    "#     if info['NAME_1'] in labeltext:\n",
    "#         x, y = zip(*shape) \n",
    "#         m.plot(x, y, marker=None,color='m',zorder = 1)\n",
    "        \n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121)\n",
    "m = Basemap(projection = 'merc',llcrnrlon=lowercorner[1],llcrnrlat=lowercorner[0],urcrnrlon=uppercorner[1],urcrnrlat=uppercorner[0], lat_ts=lowercorner[0], resolution='i',epsg=22770)\n",
    "#http://server.arcgisonline.com/arcgis/rest/services\n",
    "\n",
    "m.arcgisimage(service='ESRI_Imagery_World_2D', xpixels = 300, verbose= True)\n",
    "m.drawcoastlines(linewidth=2)\n",
    "m.drawcountries(linewidth=2,zorder=1)\n",
    "\n",
    "plotvar = df['Number of Ind'].values+df['Number of Ind in SSBs'].values\n",
    "#plotvar = df['Number of Latrines'].values\n",
    "x, y = m(df['Longitude'].values, df['Latitude'].values)\n",
    "m.hexbin(x, y, C=plotvar, reduce_C_function = np.mean, gridsize=100,  mincnt=1, cmap=cm.YlOrRd_r,zorder=2,norm=matplotlib.colors.LogNorm()); # bins='log' is log colorscale and yellow is highest (http://matplotlib.org/1.2.1/examples/pylab_examples/hexbin_demo.html)\n",
    "formatter = LogFormatter(10, labelOnlyBase=False) #,norm=matplotlib.colors.LogNorm()\n",
    "cb = plt.colorbar(format=formatter) #format=formatter\n",
    "cb.set_ticks([1,10,50,100,500], update_ticks=True)\n",
    "cb.set_ticklabels(['1','10','50','100','500'], update_ticks=True)\n",
    "cb.set_label('Mean number of people in settlements')\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "m1 = Basemap(projection = 'merc',llcrnrlon=lowercorner[1],llcrnrlat=lowercorner[0],urcrnrlon=uppercorner[1],urcrnrlat=uppercorner[0], lat_ts=lowercorner[0], resolution='i',epsg=22770)\n",
    "#http://server.arcgisonline.com/arcgis/rest/services\n",
    "\n",
    "m1.arcgisimage(service='ESRI_Imagery_World_2D', xpixels = 300, verbose= True)\n",
    "m1.drawcoastlines(linewidth=2)\n",
    "m1.drawcountries(linewidth=2,zorder=1)\n",
    "\n",
    "#plotvar = df['Number of Ind'].values+df['Number of Ind in SSBs'].values\n",
    "plotvar = df['Water Capacity in L'].values\n",
    "x, y = m1(df['Longitude'].values, df['Latitude'].values)\n",
    "m1.hexbin(x, y, C=plotvar, reduce_C_function = np.mean, gridsize=100,  mincnt=1, cmap=cm.YlOrRd_r,zorder=2,norm=matplotlib.colors.LogNorm()); # bins='log' is log colorscale and yellow is highest (http://matplotlib.org/1.2.1/examples/pylab_examples/hexbin_demo.html)\n",
    "formatter = LogFormatter(10, labelOnlyBase=False) #,norm=matplotlib.colors.LogNorm()\n",
    "cb = plt.colorbar() #format=formatter\n",
    "cb.set_ticks([1,10,100,1000,10000,50000], update_ticks=True)\n",
    "cb.set_ticklabels(['1','10','100','1000','10000','50000'], update_ticks=True)\n",
    "cb.set_label('Water capacity in L')\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "df.describe()\n",
    "\n",
    "\n",
    "# ## Exploratory analysis and data cleaning\n",
    "# * Can we predict from location data, water capacity, etc. if the **Status** is *Active* or *Less than 4*?\n",
    "# * Remove *Inactive*, *Erroneous* and *Not Willing*; set *Less than 4* to 0, and *Active* to 1\n",
    "# * Possible features to explore: **Number of Ind** (range?), **Water Capacity in L** (range), **Waste Disposal**, **Waste Water Disposal**, **Number of Latrines**, **District** or **Governorate**, Length of site creation, **Number of SSBs**, **Type of Water Source**, **Type of Contract**, **Type of Internet Connection**.\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "#drop Inactive, Erroneous and Not Willing rows from Status:\n",
    "df=df[~df['Status'].isin([\"Inactive\"])].reset_index(drop=True)\n",
    "df=df[~df['Status'].isin([\"Erroneous\"])].reset_index(drop=True)\n",
    "df=df[~df['Status'].isin([\"Not Willing\"])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# map Status onto 0/1:\n",
    "df['Status'] = df['Status'].map( {'Active': 1, 'Less than 4': 0} ).astype(int)\n",
    "df['Status'].value_counts()\n",
    "\n",
    "\n",
    "# ### Missing values\n",
    "# * Type of Water Source has 63 missing values\n",
    "# * Waste Disposal has 66 missing values\n",
    "# * Waste Water Disposal has 152 missing values\n",
    "# * Type of Contract has 99 missing values\n",
    "# * Type of Internet Connection has 450 missing values\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "df['Type of Water Source'].value_counts().sum()\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "df['Waste Disposal'].value_counts().sum()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "df['Waste Water Disposal'].value_counts().sum()\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "df['Type of Contract'].value_counts().sum()\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "df['Type of Internet Connection'].value_counts().sum()\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "#Drop NAN in all of the above (later: can use the median to replace):\n",
    "df = df.dropna(axis=0,subset=['Type of Water Source'],how='all')\n",
    "df = df.dropna(axis=0,subset=['Waste Disposal'],how='all')\n",
    "df = df.dropna(axis=0,subset=['Waste Water Disposal'],how='all')\n",
    "df = df.dropna(axis=0,subset=['Type of Contract'],how='all')\n",
    "df = df.dropna(axis=0,subset=['Type of Internet Connection'],how='all')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ### Some visualization\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(df, col='Status')\n",
    "g.map(plt.hist, 'Water Capacity in L', bins=50)\n",
    "\n",
    "\n",
    "# Water capacity close to 0 in Less than 4 category.\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(df, col='Status')\n",
    "g.map(plt.hist, 'Number of Latrines', bins=10)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(df, col='Status')\n",
    "g.map(plt.hist, 'Number of SSBs', bins=50)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(df, col='Status')\n",
    "g.map(plt.hist, 'Number of Ind in SSBs', bins=50)\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# map Waste Disposal onto 1, 2, or 3 (lump Bury it with Dump it outside the camp):\n",
    "df['Waste Disposal'] = df['Waste Disposal'].map( {'Municipality Collection': 1, 'Burn it': 2, 'Dump it outside the camp': 3, 'Burry it': 3} ).astype(int)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "df['Waste Water Disposal'].value_counts()\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "df['Waste Water Disposal'] = df['Waste Water Disposal'].replace(['Storm water channel', 'Septic tank','Municipality sewer network / treated',                                                                 'Irrigation canal'], 'Rare')\n",
    "df['Waste Water Disposal'].value_counts()\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "# map Waste Water Disposal into 1, 2, 3, 4, or 5:\n",
    "df['Waste Water Disposal'] = df['Waste Water Disposal'].map( {'Direct discharge to environment': 1, 'Cesspit': 2, 'Open pit': 3, 'Holding tank': 4, 'Municipality sewer network / not treated': 5, 'Rare': 6} ).astype(int)\n",
    "df['Waste Water Disposal'].value_counts()\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "df[['Governorate', 'Status']].groupby(['Governorate'], as_index=False).mean().sort_values(by='Status', ascending=False)\n",
    "#do not take Governorate as feature?\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "# map Waste Water Disposal into 1, 2, 3, 4, or 5:\n",
    "df['Governorate'] = df['Governorate'].map( {'Bekaa': 1, 'North': 2, 'South': 3, 'Mount Lebanon': 4, 'Nabatiye': 5, 'Beirut': 6} ).astype(int)\n",
    "df['Governorate'].value_counts()\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "df[['District', 'Status']].groupby(['District'], as_index=False).mean().sort_values(by='Status', ascending=False)\n",
    "#do not take District as feature\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "df[['Waste Disposal', 'Status']].groupby(['Waste Disposal'], as_index=False).mean().sort_values(by='Status', ascending=False)\n",
    "#do not take Waste Disposal as feature\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "df[['Waste Water Disposal', 'Status']].groupby(['Waste Water Disposal'], as_index=False).mean().sort_values(by='Status', ascending=False)\n",
    "#take Waste Water Disposal as feature?\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "df['Type of Water Source'].value_counts()\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "df['Type of Water Source'] = df['Type of Water Source'].replace(['Spring', 'Well','Others',                                                                 'River'], 'Rare')\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "df[['Type of Water Source', 'Status']].groupby(['Type of Water Source'], as_index=False).mean().sort_values(by='Status', ascending=False)\n",
    "#we *could* take this one as a feature\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "# map Type of Water Source into 1, 2, 3, or 4:\n",
    "df['Type of Water Source'] = df['Type of Water Source'].map( {'Water Trucking': 1, 'Borehole': 2, 'Water Network': 3, 'Rare': 4} ).astype(int)\n",
    "df['Type of Water Source'].value_counts()\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "df['Type of Contract'].value_counts()\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "df[['Type of Contract', 'Status']].groupby(['Type of Contract'], as_index=False).mean().sort_values(by='Status', ascending=False)\n",
    "# this is a good feature\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "# map Type of Contract into 1, 2, or 3:\n",
    "df['Type of Contract'] = df['Type of Contract'].map( {'Verbal': 1, 'None': 2, 'Written': 3} ).astype(int)\n",
    "df['Type of Contract'].value_counts()\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "df['Type of Internet Connection'].value_counts()\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "df[['Type of Internet Connection', 'Status']].groupby(['Type of Internet Connection'], as_index=False).mean().sort_values(by='Status', ascending=False)\n",
    "#possible feature\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "# map Type of Internet Connection into 1, 2, or 3:\n",
    "df['Type of Internet Connection'] = df['Type of Internet Connection'].map( {'Mobile network - 3G / 4G': 1, 'Wifi  / local Internet service provider': 2, 'No Internet access': 3} ).astype(int)\n",
    "df['Type of Internet Connection'].value_counts()\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "df[['Consultation Fee for PHC (3000/5000)', 'Status']].groupby(['Consultation Fee for PHC (3000/5000)'], as_index=False).mean().sort_values(by='Status', ascending=False)\n",
    "#not a good feature\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "df[['Free Vaccination for Children under 12', 'Status']].groupby(['Free Vaccination for Children under 12'], as_index=False).mean().sort_values(by='Status', ascending=False)\n",
    "#not a good feature\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "df['LatrinesBand'] = pd.qcut(df['Number of Latrines'], 4)\n",
    "df[['LatrinesBand', 'Status']].groupby(['LatrinesBand'], as_index=False).mean().sort_values(by='LatrinesBand', ascending=True)\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "df.loc[df['Number of Latrines'] <= 1, 'LatrinesBandNum'] = 0\n",
    "df.loc[(df['Number of Latrines'] > 1) & (df['Number of Latrines'] <= 2), 'LatrinesBandNum'] = 1\n",
    "df.loc[(df['Number of Latrines'] > 2) & (df['Number of Latrines'] <= 5), 'LatrinesBandNum']   = 2\n",
    "df.loc[ df['Number of Latrines'] > 5, 'LatrinesBandNum'] = 3\n",
    "df['LatrinesBandNum'] = df['LatrinesBandNum'].astype(int)\n",
    "df = df.drop(['LatrinesBand'], axis=1)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "df['WaterBand'] = pd.qcut(df['Water Capacity in L'], 4)\n",
    "df[['WaterBand', 'Status']].groupby(['WaterBand'], as_index=False).mean().sort_values(by='WaterBand', ascending=True)\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "df.loc[df['Water Capacity in L'] <= 1000, 'WaterBandNum'] = 0\n",
    "df.loc[(df['Water Capacity in L'] > 1000) & (df['Water Capacity in L'] <= 2500), 'WaterBandNum'] = 1\n",
    "df.loc[(df['Water Capacity in L'] > 2500) & (df['Water Capacity in L'] <= 7000), 'WaterBandNum']   = 2\n",
    "df.loc[ df['Water Capacity in L'] > 7000, 'WaterBandNum'] = 3\n",
    "df['WaterBandNum'] = df['WaterBandNum'].astype(int)\n",
    "df = df.drop(['WaterBand'], axis=1)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# ### Features to keep : \n",
    "# **Water Capacity in L** (range), **Waste Water Disposal** (maybe), **Number of Latrines**, **Type of Water Source**, **Type of Contract**, **Type of Internet Connection**.\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "df1 = df[['Status','Waste Water Disposal','Type of Water Source', 'Type of Contract','Type of Internet Connection','LatrinesBandNum','WaterBandNum','Governorate']]\n",
    "df1.head()\n",
    "\n",
    "\n",
    "# ## Machine Learning\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "df_x = df1.iloc[:,1:] #all the data except labels\n",
    "df_y = df1.iloc[:,0] #the labels\n",
    "x_train,x_test,y_train,y_test = train_test_split(df_x,df_y,test_size=0.2,random_state = 4)\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(x_train, y_train)\n",
    "y_pred = decision_tree.predict(x_test)\n",
    "acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\n",
    "acc_decision_tree\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "# k-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)\n",
    "acc_knn = round(knn.score(x_train, y_train) * 100, 2)\n",
    "acc_knn\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "RF_fit = random_forest.fit(x_train, y_train)\n",
    "#y_pred = random_forest.predict(x_test)\n",
    "y_pred = RF_fit.predict(x_test)\n",
    "random_forest.score(x_train, y_train)\n",
    "acc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\n",
    "acc_random_forest\n",
    "\n",
    "\n",
    "# ### Cross-validation\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "#CV\n",
    "scores = cross_val_score(random_forest, x_train, y_train, cv=10) #,scoring='f1_macro'\n",
    "print((\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)))\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "#transforming features\n",
    "from sklearn import preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.4, random_state=0)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "#clf = SVC(C=1).fit(X_train_transformed, y_train)\n",
    "clf = random_forest.fit(X_train_transformed, y_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "clf.score(X_test_transformed, y_test)  \n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "#using a pipeline to transform features\n",
    "from sklearn.pipeline import make_pipeline\n",
    "clf = make_pipeline(preprocessing.StandardScaler(), SVC(C=1))\n",
    "cross_val_score(clf, df_x, df_y, cv=10)\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "predicted = cross_val_predict(clf, df_x, df_y, cv=10)\n",
    "metrics.accuracy_score(df_y, predicted) \n",
    "\n",
    "\n",
    "# ### ROC curves of cross-validation folds\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "X = df_x\n",
    "y = df_y\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=6)\n",
    "#classifier = svm.SVC(kernel='linear', probability=True, random_state=random_state) \n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])\n",
    "lw = 2\n",
    "\n",
    "i = 0\n",
    "for (train, test), color in zip(cv.split(X, y), colors):\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color,\n",
    "             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',\n",
    "         label='Luck')\n",
    "\n",
    "mean_tpr /= cv.get_n_splits(X, y)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',\n",
    "         label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### Feature importance\n",
    "# The plot below suggests that two features, **Number of Latrines** and **Water Capacity in L**, are informative in this classification task.\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "coeff_df = pd.DataFrame(df1.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(random_forest.feature_importances_)\n",
    "\n",
    "print(coeff_df.sort_values(by='Correlation', ascending=False))\n",
    "\n",
    "importances = random_forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in random_forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# # Print the feature ranking\n",
    "# print(\"Feature ranking:\")\n",
    "\n",
    "# for f in range(x_train.shape[1]):\n",
    "#     print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(list(range(x_train.shape[1])), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(list(range(x_train.shape[1])), x_train.columns[indices].values, rotation='vertical')\n",
    "plt.xlim([-1, x_train.shape[1]])\n",
    "plt.ylabel('Correlation')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
