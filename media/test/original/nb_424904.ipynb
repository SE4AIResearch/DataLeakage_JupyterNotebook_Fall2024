{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "#-*-encoding:utf-8 -*-\n",
    "#!/usr/bin/python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "csv_data = '''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "0.0,11.0,12.0,'''\n",
    "csv_data = unicode(csv_data)\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "#Eliminating samples or features with missing values\n",
    "df.dropna()\n",
    "df.dropna(axis = 1)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#Replacing missing data with mean interpolation\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "# from sklearn.preprocessing import Imputer\n",
    "imr = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imr = imr.fit(df)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "'''\n",
    "Handling categoical data\n",
    "'''\n",
    "df = pd.DataFrame([['green', 'M', 10.1, 'class1'],\n",
    "                   ['red', 'L', 13.5, 'class2'],\n",
    "                   ['blue', 'XL', 15.3, 'class1']])\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "\n",
    "#Mapping ordinal features\n",
    "size_mapping = {'XL':3, 'L':2, 'M':1}\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "\n",
    "#Encoding class labels\n",
    "class_mapping = {label : idx for idx,label in enumerate(np.unique(df['classlabel']))}\n",
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "#Partitioning a dataset in training and test sets\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash','Alcalinity of ash', 'Magnesium','Total phenols', 'Flavanoids',\n",
    "                   'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "'''\n",
    "Features prepocessing includes two main methods\n",
    "One is normalization, another is standardization\n",
    "'''\n",
    "#N0rmalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)\n",
    "\n",
    "#Standardardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.transform(X_test)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "'''Selecting meaningful features\n",
    "In the overfitting case, there are 4 commmon ways\n",
    "1. More training data\n",
    "2. penalty\n",
    "3. simpler model\n",
    "4. reduce dimensionality for data\n",
    "'''\n",
    "#L1 regularization\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty = 'l1', C = 0.1)\n",
    "lr.fit(X_train_std, y_train)\n",
    "Training_accuracy = lr.score(X_train_std, y_train)\n",
    "Testing_accuracy = lr.score(X_test_std, y_test)\n",
    "\n",
    "#Regularization path\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "colors = ['blue', 'green', 'red', 'cyan','magenta', 'yellow', 'black','pink', 'lightgreen', 'lightblue','gray', 'indigo', 'orange']\n",
    "weights, params = [], []\n",
    "for c in np.arange(-4,6):\n",
    "    lr = LogisticRegression(penalty='l1', C=10**c, random_state=0)\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10**c)\n",
    "weights = np.array(weights)\n",
    "for column, color in zip(range(weights.shape[1]), colors):\n",
    "    plt.plot(params, weights[:, column],label=df_wine.columns[column+1], color=color)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=3)\n",
    "plt.xlim([10**(-5), 10**5])\n",
    "plt.ylabel('weight coefficient')\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper left')\n",
    "ax.legend(loc='upper center',bbox_to_anchor=(1.38, 1.03), ncol=1, fancybox=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "'''\n",
    "@Author: Darcy\n",
    "@Date: May, 17, 2017\n",
    "@Topic: SBS\n",
    "A classic sequential feature selection algorithm is Sequential Backward Selection (SBS)\n",
    "which aims to reduce the dimensionality of the initial feature subspace \n",
    "with a minimum decay in performance of the classifier \n",
    "to improve upon computational efficiency\n",
    "'''\n",
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "'''\n",
    "@Author: Darcy\n",
    "@Date: May, 17, 2017\n",
    "@Topic: SBS\n",
    "A classic sequential feature selection algorithm is Sequential Backward Selection (SBS)\n",
    "which aims to reduce the dimensionality of the initial feature subspace \n",
    "with a minimum decay in performance of the classifier \n",
    "to improve upon computational efficiency\n",
    "'''\n",
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class SBS:\n",
    "\tdef __init__(self, estimator, k_features,scoring=accuracy_score,\n",
    "\t\t         test_size=0.25, random_state=1):\n",
    "\t\tself.scoring = scoring\n",
    "\t\tself.estimator = clone(estimator)\n",
    "\t\tself.k_features = k_features\n",
    "\t\tself.test_size = test_size\n",
    "\t\tself.random_state = random_state\n",
    "\n",
    "\tdef fit(self, X, Y):\n",
    "\t\tx_train, y_train, x_test, y_test = \t\t\t\ttrain_test_split(X, Y, test_size = self.test_size, \n",
    "\t\t\t\t\t\t\t\t\trandom_state = self.random_state) \n",
    "\t\tdim = np.shape(x_train)[1]\n",
    "\t\tself.indices = tuple(range(dim))\n",
    "\t\tself.subSets_ = [self.indices]\n",
    "\t\tscore = self.calScore(self.x_train, y_train, x_test, y_test, self.indices)\n",
    "\t\tself.scores_ = [score]\n",
    "\t\twhile dim > self.k_features:\n",
    "\t\t\tscores = []\n",
    "\t\t\tsubSet = []\n",
    "\t\t\tfor p in combinations(self.indices, dim - 1):\n",
    "\t\t\t\tscore = self.calScore(self.x_train, y_train, x_test, y_test, p)\n",
    "\t\t\t\tscores.append(score)\n",
    "\t\t\t\tsubSet.append(p)\n",
    "\t\t\tbest = np.argmax(score)\n",
    "\t\t\tself.indices = subSet[best]\n",
    "\t\t\tself.subSets_.append(self.indices)\n",
    "\t\t\tdim -= 1\n",
    "\t\t\tself.scores_.append(scores[best])\n",
    "\t\tself.k_score = self.scores_[-1]\n",
    "\t\treturn self\n",
    "\n",
    "\n",
    "\tdef calScore(self, x_train, y_train, x_test, y_test, indices):\n",
    "\t\tself.estimator.fit(x_train, y_train)\n",
    "\t\ty_pred = self.estimator.predict(x_test)\n",
    "\t\tscore = self.scoring(y_test, y_pred)\n",
    "\t\treturn score\n",
    "\n",
    "class SBS():\n",
    "\tdef __init__(self, estimator, k_features,\n",
    "            scoring=accuracy_score,\n",
    "            test_size=0.25, random_state=1):\n",
    "\t\tself.scoring = scoring\n",
    "\t\tself.estimator = clone(estimator)\n",
    "\t\tself.k_features = k_features\n",
    "\t\tself.test_size = test_size\n",
    "\t\tself.random_state = random_state\n",
    "        \n",
    "\tdef fit(self, X, y):\n",
    "\t\tX_train, X_test, y_train, y_test = \t\ttrain_test_split(X, y, test_size=self.test_size,\n",
    "\t\trandom_state=self.random_state)\n",
    "\t\tdim = X_train.shape[1]\n",
    "\t\tself.indices_ = tuple(range(dim))\n",
    "\t\tself.subsets_ = [self.indices_]\n",
    "\t\tscore = self._calc_score(X_train, y_train,\n",
    "\t\tX_test, y_test, self.indices_)\n",
    "\t\tself.scores_ = [score]\n",
    "\t\twhile dim > self.k_features:\n",
    "\t\t\tscores = []\n",
    "\t\t\tsubsets = []\n",
    "\t\t\tfor p in combinations(self.indices_, r=dim-1):\n",
    "\t\t\t\tscore = self._calc_score(X_train, y_train,\n",
    "\t\t\t\tX_test, y_test, p)\n",
    "\t\t\t\tscores.append(score)\n",
    "\t\t\t\tsubsets.append(p)\n",
    "\t\t\t\tbest = np.argmax(scores)\n",
    "\t\t\t\tself.indices_ = subsets[best]\n",
    "\t\t\t\tself.subsets_.append(self.indices_)\n",
    "\t\t\tdim -= 1\n",
    "\t\t\tself.scores_.append(scores[best])\n",
    "\t\tself.k_score_ = self.scores_[-1]\n",
    "\t\treturn self\n",
    "\n",
    "\tdef _calc_score(self, x_train, y_train, x_test, y_test, indices):\n",
    "\t\tself.estimator.fit(x_train, y_train)\n",
    "\t\ty_pred = self.estimator.predict(x_test)\n",
    "\t\tscore = self.scoring(y_test, y_pred)\n",
    "\t\treturn score\n",
    "\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "sbs = SBS(knn, k_features=1)\n",
    "sbs.fit(X_train_std, y_train)\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "k_fea = [len(k) for k in sbs.subsets_]\n",
    "plt.plot(k_fea, sbs.scores_, marker = 'o')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
