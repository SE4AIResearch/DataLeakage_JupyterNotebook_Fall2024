{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "import patsy\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'sql')\n",
    "\n",
    "\n",
    "# ## Pre-Task: Describe the goals of your study\n",
    "\n",
    "# Wealthier Women and children had a higher chance of surviving\n",
    "\n",
    "#   \n",
    "\n",
    "# ## Part 1: Aquire the Data\n",
    "\n",
    "# psql -h dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com -p 5432 -U dsi_student titanic\n",
    "# password: gastudents\n",
    "# CONNECTED IN TERMINAL\n",
    "\n",
    "# #### 1. Connect to the remote database\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#import remote data into python notebook\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://dsi_student:gastudents@dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com/titanic')\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#create dataframe in pandas from titanic data\n",
    "df = pd.read_sql('SELECT * FROM train', engine)\n",
    "\n",
    "\n",
    "# #### 2. Query the database and aggregate the data\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# #### 5. What are the risks and assumptions of our data? \n",
    "\n",
    "# Assume that the data without age can be dropped, assume that this is representative of all data. Assume that survived is accurate. \n",
    "\n",
    "# ## Part 2: Exploratory Data Analysis\n",
    "\n",
    "# #### 1. Describe the Data\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "df.describe()\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "# #### 2. Visualize the Data\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.clustermap(df.corr(), annot=True)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.pairplot(df, x_vars=[\"Survived\"], y_vars=[\"Age\", \"Parch\", \"Pclass\"], size=6)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "sns.pairplot(df.dropna())\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "df['AgeRounded']=[round(x) for x in df['Age']]\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "df['AgeRounded'].unique()\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "survived_age = df[[\"AgeRounded\", \"Survived\"]].groupby(['AgeRounded'],as_index=False).mean()\n",
    "survived_age\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "sns.set(font_scale=2.2)\n",
    "plt.figure(figsize=(45,30))\n",
    "\n",
    "sns.barplot(x='AgeRounded', y='Survived', data=survived_age)\n",
    "\n",
    "\n",
    "# ## Part 3: Data Wrangling\n",
    "\n",
    "# #### 1. Create Dummy Variables for *Sex* \n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "df['female']=[1 if x=='female' else 0 for x in df['Sex']]\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# ## Part 4: Logistic Regression and Model Validation\n",
    "\n",
    "# #### 1. Define the variables that we will use in our classification analysis\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "df=df.dropna(subset=['Age'])\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "'''according to National Statistical Standards the age distribution of the population for demographic purposes should be\n",
    "given in five-year age groups extending to 85 years and over.'''\n",
    "#May try another one with smaller groups\n",
    "\n",
    "bins = [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84]\n",
    "#highest age is 80\n",
    "group_names =['0-4 years', '5-9 years','10-14 years', '15-19 years', '20-24 years', '25-29 years', '30-34 years','35-39 years', '40-44 years', '45-49 years', '50-54 years', '55-59 years', '60-64 years', '65-69 years','70-74 years', '75-79 years', '80-84 years']\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "#create new column with age labels\n",
    "df['agebin'] = pd.cut(df['Age'], bins, labels=group_names)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "#define features\n",
    "features=df[['agebin','Sex','Pclass', 'Parch', 'Fare' ]]\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "farelist=df['Fare'].unique()\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "sorted(farelist)\n",
    "#decide to note include fare as a predictor\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "#define Predictors in categorical contexts\n",
    "X= patsy.dmatrix('~ C(agebin) + C(Sex) + C(Pclass)+ C(Parch)', df)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "X\n",
    "\n",
    "\n",
    "# #### 2. Transform \"Y\" into a 1-Dimensional Array for SciKit-Learn\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "#Define category that will be predicted\n",
    "y=df['Survived'].values\n",
    "\n",
    "\n",
    "# #### 3. Conduct the logistic regression\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "#DECIDED NOT TO SCALE BECUASE THERE ARE NO CONTINUOUS VARIABLES\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "#split data into test and train\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=.33, random_state=66)\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "#create 'vanilla' linear model\n",
    "lr = LogisticRegression(solver='liblinear') \n",
    "lr_model = lr.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# #### 4. Examine the coefficients to see our correlations\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "lr_model.coef_[0]\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "featurenames=X.design_info.column_names\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "coeff_logreg= pd.DataFrame(list(zip(featurenames, lr_model.coef_[0])))\n",
    "coeff_logreg.columns= ['Feature_Name', 'Coefficient']\n",
    "coeff_logreg\n",
    "\n",
    "\n",
    "# #### 6. Test the Model by introducing a *Test* or *Validaton* set \n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "#Done ABOVE\n",
    "\n",
    "\n",
    "# #### 7. Predict the class labels for the *Test* set\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "#predict the survival for X_test \n",
    "y_pred= lr_model.predict(X_test)\n",
    "\n",
    "\n",
    "# #### 8. Predict the class probabilities for the *Test* set\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "lr_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# #### 9. Evaluate the *Test* set\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "lr_model.score(X_test, Y_test)\n",
    "\n",
    "\n",
    "# #### 10. Cross validate the test set\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "cross_val_score(lr_model, X_test, Y_test, cv=3, scoring='f1_weighted').mean()\n",
    "\n",
    "\n",
    "# #### 11. Check the Classification Report\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "print(classification_report(Y_test, y_pred, labels=lr_model.classes_))\n",
    "\n",
    "\n",
    "# #### 12. What do the classification metrics tell us?\n",
    "\n",
    "# #F1 score is weighted average of the precision and recall. This is where 1 is best and 0 is worst. \n",
    "\n",
    "# #### 13. Check the Confusion Matrix\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred, labels=lr_model.classes_)\n",
    "cm = pd.DataFrame(cm, columns=lr_model.classes_, index=lr_model.classes_)\n",
    "cm\n",
    "\n",
    "\n",
    "# #### 14. What does the Confusion Matrix tell us? \n",
    "\n",
    "# Errors of predicted survival tend to occur 18% of the time. Specifically, there were 15 occurances of a false positive and 28 occurances of false negatives\n",
    "\n",
    "# #### 15. Plot the ROC curve\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "#get score for the y prediction\n",
    "y_score = lr_model.decision_function(X_test)\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "FPR = dict()\n",
    "TPR = dict()\n",
    "ROC_AUC = dict()\n",
    "\n",
    "# For class 1, find the area under the curve\n",
    "FPR[1], TPR[1], _ = roc_curve(Y_test, y_score)\n",
    "ROC_AUC[1] = auc(FPR[1], TPR[1])\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR[1], TPR[1], label='ROC curve (area = %0.2f)' % ROC_AUC[1], linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for high/low income', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# #### 16. What does the ROC curve tell us?\n",
    "\n",
    "# \"This means that the top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one.\"-http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "# ## Part 5: Gridsearch\n",
    "\n",
    "# #### 1. Use GridSearchCV with logistic regression to search for optimal parameters \n",
    "# \n",
    "# - Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n",
    "# - Use 5-fold cross-validation.\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "logreg_parameters = {\n",
    "    'penalty':['l1','l2'],\n",
    "    'C':np.logspace(-5,1,50),\n",
    "    'solver':['liblinear']\n",
    "}\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "#run gridsearch on the dict\n",
    "gs = GridSearchCV(lr_model, {'penalty': logreg_parameters['penalty'], 'C': logreg_parameters['C']}, verbose=False, cv=15)\n",
    "gs.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "#return best model parameters\n",
    "gs.best_params_\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "#return best score\n",
    "gs.best_score_\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "#fit model with gridsearch info\n",
    "logreg = LogisticRegression(C=gs.best_params_['C'], penalty=gs.best_params_['penalty'])\n",
    "gs_model = logreg.fit(X_train, Y_train)\n",
    "\n",
    " \n",
    "#predict y\n",
    "gs_pred = gs_model.predict(X_test)\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "#create confusion matrix\n",
    "cm1 = confusion_matrix(Y_test, gs_pred, labels=logreg.classes_)\n",
    "cm1 = pd.DataFrame(cm1, columns=logreg.classes_, index=logreg.classes_)\n",
    "cm1\n",
    "\n",
    "\n",
    "# #### 2. Print out the best parameters and best score. Are they better than the vanilla logistic regression?\n",
    "\n",
    "# Seen above... It seems as though the false positive rate grew in the gridsearch model and the false negative fell. The model now errors on the side of saying someone survived. \n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "y_score2 = gs_model.decision_function(X_test)\n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "FPR_GS = dict()\n",
    "TPR_GS = dict()\n",
    "ROC_AUC_GS = dict()\n",
    "\n",
    "# For class 1, find the area under the curve\n",
    "FPR_GS[1], TPR_GS[1], _ = roc_curve(Y_test, y_score2)\n",
    "ROC_AUC_GS[1] = auc(FPR_GS[1], TPR_GS[1])\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR_GS[1], TPR_GS[1], label='ROC curve (area = %0.2f)' % ROC_AUC_GS[1], linewidth=4)\n",
    "plt.plot(FPR[1], TPR[1], label='ROC curve (area = %0.2f)' % ROC_AUC[1], linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for high/low income', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# #### 3. Explain the difference between the difference between the L1 (Lasso) and L2 (Ridge) penalties on the model coefficients.\n",
    "\n",
    "# L1 tends to drop one variable. In a way the coefficient becomes 0. L2(Ridge) tends to change the coefficients in a smaller way.\n",
    "\n",
    "# #### 4. What hypothetical situations are the Ridge and Lasso penalties useful?\n",
    "\n",
    "# Use Lasso when there are too many variables and you would like to remove one/some. Ridge is used when you would like to find a middle ground between variables. \n",
    "\n",
    "# #### 5. [BONUS] Explain how the regularization strength (C) modifies the regression loss function. Why do the Ridge and Lasso penalties have their respective effects on the coefficients?\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 6.a. [BONUS] You decide that you want to minimize false positives. Use the predicted probabilities from the model to set your threshold for labeling the positive class to need at least 90% confidence. How and why does this affect your confusion matrix?\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Part 6: Gridsearch and kNN\n",
    "\n",
    "# #### 1. Perform Gridsearch for the same classification problem as above, but use KNeighborsClassifier as your estimator\n",
    "# \n",
    "# At least have number of neighbors and weights in your parameters dictionary.\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "#create dictionary for gridsearch\n",
    "param_dict = dict(n_neighbors=list(range(1, 51)),                  weights=['uniform', 'distance'])\n",
    "\n",
    "\n",
    "# #### 2. Print the best parameters and score for the gridsearched kNN model. How does it compare to the logistic regression model?\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "#Perform gridsearch\n",
    "gscv = GridSearchCV(knn, param_dict, scoring='accuracy')\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "#fit model based upon gridsearch\n",
    "gscv_model = gscv.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "#return estimator info\n",
    "gscv_model.best_estimator_.get_params()\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "#return best parameters\n",
    "gscv.best_params_\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "\n",
    "#return score for these parameters\n",
    "gscv.best_score_\n",
    "\n",
    "\n",
    "# #### 3. How does the number of neighbors affect the bias-variance tradeoff of your model?\n",
    "# \n",
    "# #### [BONUS] Why?\n",
    "\n",
    "# The lower neighbors the possibility of a neighbor skewing the result is higher. Too high of neighbors could yeild just a majority of the data set\n",
    "\n",
    "# #### 4. In what hypothetical scenario(s) might you prefer logistic regression over kNN, aside from model performance metrics?\n",
    "\n",
    "# the data doesnt seem to have a clear split between classification\n",
    "\n",
    "# #### 5. Fit a new kNN model with the optimal parameters found in gridsearch. \n",
    "\n",
    "# In[69]:\n",
    "\n",
    "\n",
    "#model fit above with parameters\n",
    "\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "\n",
    "gscv_ypred = gscv_model.predict(X_test)\n",
    "\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "\n",
    "print(classification_report(Y_test, gscv_ypred))\n",
    "\n",
    "\n",
    "# #### 6. Construct the confusion matrix for the optimal kNN model. Is it different from the logistic regression model? If so, how?\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "\n",
    "cm2 = confusion_matrix(Y_test, gscv_ypred)\n",
    "cm2 = pd.DataFrame(cm2)\n",
    "cm2\n",
    "\n",
    "\n",
    "# In[72]:\n",
    "\n",
    "\n",
    "#looks similar to gridsearch done with logistic regression\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 7. [BONUS] Plot the ROC curves for the optimized logistic regression model and the optimized kNN model on the same plot.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Part 7: [BONUS] Precision-recall\n",
    "\n",
    "# #### 1. Gridsearch the same parameters for logistic regression but change the scoring function to 'average_precision'\n",
    "# \n",
    "# `'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 2. Examine the best parameters and score. Are they different than the logistic regression gridsearch in part 5?\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 3. Create the confusion matrix. Is it different than when you optimized for the accuracy? If so, why would this be?\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 4. Plot the precision-recall curve. What does this tell us as opposed to the ROC curve?\n",
    "# \n",
    "# [See the sklearn plotting example here.](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Part 8: [VERY BONUS] Decision trees, ensembles, bagging\n",
    "\n",
    "# #### 1. Gridsearch a decision tree classifier model on the data, searching for optimal depth. Create a new decision tree model with the optimal parameters.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 2. Compare the performace of the decision tree model to the logistic regression and kNN models.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 3. Plot all three optimized models' ROC curves on the same plot. \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 4. Use sklearn's BaggingClassifier with the base estimator your optimized decision tree model. How does the performance compare to the single decision tree classifier?\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 5. Gridsearch the optimal n_estimators, max_samples, and max_features for the bagging classifier.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### 6. Create a bagging classifier model with the optimal parameters and compare it's performance to the other two models.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
