{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[109]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "\n",
    "# create graph: model\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b\n",
    "\n",
    "# create graph: loss\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "\n",
    "# bind optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# run graph\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Fit the line.\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(W), sess.run(b))\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "# In[128]:\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,), activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "# In[189]:\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "# In[190]:\n",
    "\n",
    "\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# In[191]:\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# In[192]:\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import keras\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# In[193]:\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "num_classes = 10\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# In[194]:\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1)\n",
    "\n",
    "\n",
    "# In[197]:\n",
    "\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss: {:.3f}\".format(score[0]))\n",
    "print(\"Test Accuracy: {:.3f}\".format(score[1]))\n",
    "\n",
    "\n",
    "# In[134]:\n",
    "\n",
    "\n",
    "# recreating the model seems the only way to reset?\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# In[135]:\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[136]:\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,), activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history_callback = model.fit(X_train, y_train, batch_size=128,\n",
    "                             epochs=100, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[164]:\n",
    "\n",
    "\n",
    "def plot_history(logger):\n",
    "    df = pd.DataFrame(logger.history)\n",
    "    df[['acc', 'val_acc']].plot()\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    df[['loss', 'val_loss']].plot(linestyle='--', ax=plt.twinx())\n",
    "    plt.ylabel(\"loss\")\n",
    "\n",
    "\n",
    "# In[144]:\n",
    "\n",
    "\n",
    "df = pd.DataFrame(history_callback.history)\n",
    "df[['acc', 'val_acc']].plot()\n",
    "plt.ylabel(\"accuracy\")\n",
    "df[['loss', 'val_loss']].plot(linestyle='--', ax=plt.twinx())\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def make_model(optimizer=\"adam\", hidden_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(hidden_size, input_shape=(784,)),\n",
    "        Activation('relu'),\n",
    "        Dense(10),\n",
    "        Activation('softmax'),\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "clf = KerasClassifier(make_model)\n",
    "\n",
    "param_grid = {'epochs': [1, 5, 10],  # epochs is fit parameter, not in make_model!\n",
    "              'hidden_size': [32, 64, 256]}\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=5)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "res = pd.DataFrame(grid.cv_results_)\n",
    "res.pivot_table(index=[\"param_epochs\", \"param_hidden_size\"],\n",
    "                values=['mean_train_score', \"mean_test_score\"])\n",
    "\n",
    "\n",
    "# In[165]:\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1024, input_shape=(784,), activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=128,\n",
    "                    epochs=20, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[186]:\n",
    "\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "# In[187]:\n",
    "\n",
    "\n",
    "score\n",
    "\n",
    "\n",
    "# In[179]:\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# In[178]:\n",
    "\n",
    "\n",
    "df = pd.DataFrame(history.history)\n",
    "df[['acc', 'val_acc']].plot()\n",
    "plt.ylabel(\"accuracy\")\n",
    "df[['loss', 'val_loss']].plot(linestyle='--', ax=plt.twinx())\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "\n",
    "# In[163]:\n",
    "\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model_dropout = Sequential([\n",
    "    Dense(1024, input_shape=(784,), activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "model_dropout.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history_dropout = model_dropout.fit(X_train, y_train, batch_size=128,\n",
    "                            epochs=20, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[152]:\n",
    "\n",
    "\n",
    "df = pd.DataFrame(history_dropout.history)\n",
    "df[['acc', 'val_acc']].plot()\n",
    "plt.ylabel(\"accuracy\")\n",
    "df[['loss', 'val_loss']].plot(linestyle='--', ax=plt.twinx())\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "\n",
    "# In[166]:\n",
    "\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "# In[167]:\n",
    "\n",
    "\n",
    "score\n",
    "\n",
    "\n",
    "# # Batch Normalization\n",
    "\n",
    "# In[198]:\n",
    "\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "model_bn = Sequential([\n",
    "    Dense(512, input_shape=(784,)),\n",
    "    BatchNormalization(),\n",
    "    Activation(\"relu\"),\n",
    "    Dense(512),\n",
    "    BatchNormalization(),\n",
    "    Activation(\"relu\"),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "model_bn.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history_bn = model.fit(X_train, y_train, batch_size=128,\n",
    "                    epochs=10, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[199]:\n",
    "\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "\n",
    "# # Convolutions\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "rng = np.random.RandomState(2)\n",
    "signal = np.cumsum(rng.normal(size=200))\n",
    "plt.plot(signal)\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "\n",
    "gaussian_filter = np.exp(-np.linspace(-2, 2, 15) ** 2)\n",
    "gaussian_filter /= gaussian_filter.sum()\n",
    "plt.plot(gaussian_filter)\n",
    "gaussian_filter\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "plt.plot(signal)\n",
    "plt.plot(convolve(signal, gaussian_filter))\n",
    "\n",
    "\n",
    "# In[110]:\n",
    "\n",
    "\n",
    "from scipy.misc import imread\n",
    "image = imread(\"IMG_20170207_090931.jpg\")\n",
    "plt.imshow(image)\n",
    "\n",
    "\n",
    "# In[111]:\n",
    "\n",
    "\n",
    "gaussian_2d = gaussian_filter * gaussian_filter[:, np.newaxis]\n",
    "plt.matshow(gaussian_2d)\n",
    "\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "\n",
    "out = convolve(image, gaussian_2d[:, :, np.newaxis])\n",
    "\n",
    "\n",
    "# In[112]:\n",
    "\n",
    "\n",
    "plt.imshow(out)\n",
    "\n",
    "\n",
    "# In[118]:\n",
    "\n",
    "\n",
    "gray_image = image.mean(axis=2)\n",
    "plt.imshow(gray_image, cmap=\"gray\")\n",
    "\n",
    "\n",
    "# In[125]:\n",
    "\n",
    "\n",
    "gradient_2d = convolve(gaussian_2d, [[-1, 1]])\n",
    "\n",
    "\n",
    "# In[126]:\n",
    "\n",
    "\n",
    "plt.imshow(gradient_2d)\n",
    "\n",
    "\n",
    "# In[127]:\n",
    "\n",
    "\n",
    "edges = convolve(gray_image, gradient_2d)\n",
    "plt.imshow(edges, cmap=\"gray\")\n",
    "\n",
    "\n",
    "# # CNN\n",
    "\n",
    "# In[174]:\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "X_train_images = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test_images = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# In[175]:\n",
    "\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "num_classes = 10\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(64, activation='relu'))\n",
    "cnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[177]:\n",
    "\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "# In[176]:\n",
    "\n",
    "\n",
    "cnn.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history_cnn = cnn.fit(X_train_images, y_train,\n",
    "                      batch_size=128, epochs=20, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[188]:\n",
    "\n",
    "\n",
    "plot_history(history_cnn)\n",
    "\n",
    "\n",
    "# In[200]:\n",
    "\n",
    "\n",
    "cnn.evaluate(X_test_images, y_test)\n",
    "\n",
    "\n",
    "# In[202]:\n",
    "\n",
    "\n",
    "df = pd.DataFrame(history_cnn.history)\n",
    "df[['acc', 'val_acc']].plot()\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(.9, 1)\n",
    "\n",
    "\n",
    "# In[205]:\n",
    "\n",
    "\n",
    "layer1 = cnn.layers[0]\n",
    "\n",
    "\n",
    "# In[214]:\n",
    "\n",
    "\n",
    "weights, biases = layer1.get_weights()\n",
    "\n",
    "\n",
    "# In[215]:\n",
    "\n",
    "\n",
    "weights.shape\n",
    "\n",
    "\n",
    "# In[217]:\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4, 6)\n",
    "for ax, weight in zip(axes.ravel(), weights.T):\n",
    "    ax.imshow(weight[0, :, :])\n",
    "\n",
    "\n",
    "# In[218]:\n",
    "\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "num_classes = 10\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(8, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(8, (5, 5), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(64, activation='relu'))\n",
    "cnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[232]:\n",
    "\n",
    "\n",
    "cnn.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history_cnn = cnn.fit(X_train_images, y_train,\n",
    "                      batch_size=128, epochs=10, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[233]:\n",
    "\n",
    "\n",
    "weights, biases = cnn.layers[0].get_weights()\n",
    "fig, axes = plt.subplots(2, 4)\n",
    "mi, ma = weights.min(), weights.max()\n",
    "for ax, weight in zip(axes.ravel(), weights.T):\n",
    "    ax.imshow(weight[0, :, :].T, vmin=mi, vmax=ma)\n",
    "\n",
    "\n",
    "# In[223]:\n",
    "\n",
    "\n",
    "weights.shape\n",
    "\n",
    "\n",
    "# In[226]:\n",
    "\n",
    "\n",
    "plt.imshow(weights[:, :, 0, 0])\n",
    "\n",
    "\n",
    "# In[236]:\n",
    "\n",
    "\n",
    "asdf = cnn.get_input_at(0)\n",
    "\n",
    "\n",
    "# In[262]:\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# with a Sequential model\n",
    "get_1rd_layer_output = K.function([cnn.layers[0].input],\n",
    "                                  [cnn.layers[0].output])\n",
    "get_3rd_layer_output = K.function([cnn.layers[0].input],\n",
    "                                  [cnn.layers[3].output])\n",
    "\n",
    "layer1_output = get_1rd_layer_output([X_train_images[:5]])[0]\n",
    "layer3_output = get_3rd_layer_output([X_train_images[:5]])[0]\n",
    "\n",
    "\n",
    "# In[263]:\n",
    "\n",
    "\n",
    "layer1_output.shape\n",
    "\n",
    "\n",
    "# In[264]:\n",
    "\n",
    "\n",
    "layer3_output.shape\n",
    "\n",
    "\n",
    "# In[267]:\n",
    "\n",
    "\n",
    "weights, biases = cnn.layers[0].get_weights()\n",
    "n_images = layer1_output.shape[0]\n",
    "n_filters = layer1_output.shape[3]\n",
    "fig, axes = plt.subplots(n_images * 2, n_filters + 1, subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i in range(layer1_output.shape[0]):\n",
    "    # for reach input image (= 2 rows)\n",
    "    axes[2 * i, 0].imshow(X_train_images[i, :, :, 0], cmap=\"gray_r\")\n",
    "    axes[2 * i + 1, 0].set_visible(False)\n",
    "    for j in range(layer1_output.shape[3]):\n",
    "        # for each feature map (same number in layer 1 and 3)\n",
    "        axes[2 * i, j + 1].imshow(layer1_output[i, :, :, j], cmap='gray_r')\n",
    "        axes[2 * i + 1, j + 1].imshow(layer3_output[i, :, :, j], cmap='gray_r')\n",
    "\n",
    "\n",
    "# In[268]:\n",
    "\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "num_classes = 10\n",
    "cnn_small = Sequential()\n",
    "cnn_small.add(Conv2D(8, kernel_size=(3, 3),\n",
    "              activation='relu',\n",
    "              input_shape=input_shape))\n",
    "cnn_small.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_small.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "cnn_small.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_small.add(Flatten())\n",
    "cnn_small.add(Dense(64, activation='relu'))\n",
    "cnn_small.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[286]:\n",
    "\n",
    "\n",
    "cnn_small.summary()\n",
    "\n",
    "\n",
    "# In[270]:\n",
    "\n",
    "\n",
    "cnn_small.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history_cnn_small = cnn_small.fit(X_train_images, y_train,\n",
    "                      batch_size=128, epochs=10, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[290]:\n",
    "\n",
    "\n",
    "weights, biases = cnn_small.layers[0].get_weights()\n",
    "weights2, biases2 = cnn_small.layers[2].get_weights()\n",
    "print(weights.shape)\n",
    "print(weights2.shape)\n",
    "\n",
    "\n",
    "# In[296]:\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(9, 8, figsize=(10, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "mi, ma = weights.min(), weights.max()\n",
    "for ax, weight in zip(axes[0], weights.T):\n",
    "    ax.imshow(weight[0, :, :].T, vmin=mi, vmax=ma)\n",
    "axes[0, 0].set_ylabel(\"layer1\")\n",
    "mi, ma = weights2.min(), weights2.max()\n",
    "for i in range(1, 9):\n",
    "    axes[i, 0].set_ylabel(\"layer3\")\n",
    "for ax, weight in zip(axes[1:].ravel(), weights2.reshape(3, 3, -1).T):\n",
    "    ax.imshow(weight[:, :].T, vmin=mi, vmax=ma)\n",
    "\n",
    "\n",
    "# In[281]:\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "get_1rd_layer_output = K.function([cnn_small.layers[0].input],\n",
    "                                  [cnn_small.layers[0].output])\n",
    "get_3rd_layer_output = K.function([cnn_small.layers[0].input],\n",
    "                                  [cnn_small.layers[2].output])\n",
    "\n",
    "layer1_output = get_1rd_layer_output([X_train_images[:5]])[0]\n",
    "layer3_output = get_3rd_layer_output([X_train_images[:5]])[0]\n",
    "\n",
    "\n",
    "# In[282]:\n",
    "\n",
    "\n",
    "layer1_output.shape\n",
    "\n",
    "\n",
    "# In[283]:\n",
    "\n",
    "\n",
    "layer3_output.shape\n",
    "\n",
    "\n",
    "# In[284]:\n",
    "\n",
    "\n",
    "weights, biases = cnn.layers[0].get_weights()\n",
    "n_images = layer1_output.shape[0]\n",
    "n_filters = layer1_output.shape[3]\n",
    "fig, axes = plt.subplots(n_images * 2, n_filters + 1, figsize=(10, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i in range(layer1_output.shape[0]):\n",
    "    # for reach input image (= 2 rows)\n",
    "    axes[2 * i, 0].imshow(X_train_images[i, :, :, 0], cmap=\"gray_r\")\n",
    "    axes[2 * i + 1, 0].set_visible(False)\n",
    "    axes[2 * i, 1].set_ylabel(\"layer1\")\n",
    "    axes[2 * i + 1, 1].set_ylabel(\"layer3\")\n",
    "    for j in range(layer1_output.shape[3]):\n",
    "        # for each feature map (same number in layer 1 and 3)\n",
    "        axes[2 * i, j + 1].imshow(layer1_output[i, :, :, j], cmap='gray_r')\n",
    "        axes[2 * i + 1, j + 1].imshow(layer3_output[i, :, :, j], cmap='gray_r')\n",
    "\n",
    "\n",
    "# # Batch Normalization\n",
    "\n",
    "# In[346]:\n",
    "\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "num_classes = 10\n",
    "cnn_small_bn = Sequential()\n",
    "cnn_small_bn.add(Conv2D(8, kernel_size=(3, 3),\n",
    "                 input_shape=input_shape))\n",
    "cnn_small_bn.add(Activation(\"relu\"))\n",
    "cnn_small_bn.add(BatchNormalization())\n",
    "cnn_small_bn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_small_bn.add(Conv2D(8, (3, 3)))\n",
    "cnn_small_bn.add(Activation(\"relu\"))\n",
    "cnn_small_bn.add(BatchNormalization())\n",
    "cnn_small_bn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_small_bn.add(Flatten())\n",
    "cnn_small_bn.add(Dense(64, activation='relu'))\n",
    "cnn_small_bn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[347]:\n",
    "\n",
    "\n",
    "cnn_small_bn.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history_cnn_small_bn = cnn_small_bn.fit(X_train_images, y_train,\n",
    "                                        batch_size=128, epochs=10, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[362]:\n",
    "\n",
    "\n",
    "hist_small_bn = pd.DataFrame(history_cnn_small_bn.history)\n",
    "hist_small = pd.DataFrame(history_cnn_small.history)\n",
    "hist_small_bn.rename(columns=lambda x: x + \" BN\", inplace=True)\n",
    "hist_small_bn[['acc BN', 'val_acc BN']].plot()\n",
    "hist_small[['acc', 'val_acc']].plot(ax=plt.gca(), linestyle='--', color=[plt.cm.Vega10(0), plt.cm.Vega10(1)])\n",
    "\n",
    "\n",
    "# In[366]:\n",
    "\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "num_classes = 10\n",
    "cnn32 = Sequential()\n",
    "cnn32.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 input_shape=input_shape))\n",
    "cnn32.add(Activation(\"relu\"))\n",
    "#cnn_small_bn.add(BatchNormalization())\n",
    "cnn32.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn32.add(Conv2D(32, (3, 3)))\n",
    "cnn32.add(Activation(\"relu\"))\n",
    "#cnn_small_bn.add(BatchNormalization())\n",
    "cnn32.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn32.add(Flatten())\n",
    "cnn32.add(Dense(64, activation='relu'))\n",
    "cnn32.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[367]:\n",
    "\n",
    "\n",
    "cnn32.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history_cnn_32 = cnn32.fit(X_train_images, y_train,\n",
    "                            batch_size=128, epochs=10, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[368]:\n",
    "\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "num_classes = 10\n",
    "cnn32_bn = Sequential()\n",
    "cnn32_bn.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 input_shape=input_shape))\n",
    "cnn32_bn.add(Activation(\"relu\"))\n",
    "cnn32_bn.add(BatchNormalization())\n",
    "cnn32_bn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn32_bn.add(Conv2D(32, (3, 3)))\n",
    "cnn32_bn.add(Activation(\"relu\"))\n",
    "cnn32_bn.add(BatchNormalization())\n",
    "cnn32_bn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn32_bn.add(Flatten())\n",
    "cnn32_bn.add(Dense(64, activation='relu'))\n",
    "cnn32_bn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[369]:\n",
    "\n",
    "\n",
    "cnn32_bn.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "history_cnn_32_bn = cnn32_bn.fit(X_train_images, y_train,\n",
    "                                 batch_size=128, epochs=10, verbose=1, validation_split=.1)\n",
    "\n",
    "\n",
    "# In[371]:\n",
    "\n",
    "\n",
    "hist_32_bn = pd.DataFrame(history_cnn_32_bn.history)\n",
    "hist_32 = pd.DataFrame(history_cnn_32.history)\n",
    "hist_32_bn.rename(columns=lambda x: x + \" BN\", inplace=True)\n",
    "hist_32_bn[['acc BN', 'val_acc BN']].plot()\n",
    "hist_32[['acc', 'val_acc']].plot(ax=plt.gca(), linestyle='--', color=[plt.cm.Vega10(0), plt.cm.Vega10(1)])\n",
    "plt.ylim(.8, 1)\n",
    "\n",
    "\n",
    "# # loading VGG\n",
    "\n",
    "# In[298]:\n",
    "\n",
    "\n",
    "from keras import applications\n",
    "\n",
    "# build the VGG16 network\n",
    "model = applications.VGG16(include_top=False,\n",
    "                           weights='imagenet')\n",
    "\n",
    "\n",
    "# In[311]:\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# In[339]:\n",
    "\n",
    "\n",
    "vgg_weights, vgg_biases = model.layers[1].get_weights()\n",
    "vgg_weights.shape\n",
    "\n",
    "\n",
    "# In[307]:\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "mi, ma = vgg_weights.min(), vgg_weights.max()\n",
    "for ax, weight in zip(axes.ravel(), vgg_weights.T):\n",
    "    ax.imshow(weight.T)\n",
    "\n",
    "\n",
    "# In[340]:\n",
    "\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "\n",
    "# In[333]:\n",
    "\n",
    "\n",
    "get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[3].output])\n",
    "get_6rd_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[6].output])\n",
    "\n",
    "layer3_output = get_3rd_layer_output([[image]])[0]\n",
    "layer6_output = get_6rd_layer_output([[image]])[0]\n",
    "\n",
    "\n",
    "# In[334]:\n",
    "\n",
    "\n",
    "print(layer3_output.shape)\n",
    "print(layer6_output.shape)\n",
    "\n",
    "\n",
    "# In[341]:\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(10, 4), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for ax, activation in zip(axes.ravel(), layer3_output.T):\n",
    "    ax.imshow(activation[:, :, 0].T, cmap=\"gray_r\")\n",
    "plt.suptitle(\"after first pooling layer\")\n",
    "\n",
    "\n",
    "# In[343]:\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(10, 4), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for ax, activation in zip(axes.ravel(), layer6_output.T):\n",
    "    ax.imshow(activation[:, :, 0].T, cmap=\"gray_r\")\n",
    "plt.suptitle(\"after second pooling layer\")\n",
    "\n",
    "\n",
    "# In[384]:\n",
    "\n",
    "\n",
    "import flickrapi\n",
    "import json\n",
    "\n",
    "\n",
    "api_key = u'f770a9e7064fa7f8754b1ed8cc8cda4f'\n",
    "api_secret = u' 2e750f2d723350c8 '\n",
    "\n",
    "import flickrapi\n",
    "flickr = flickrapi.FlickrAPI(api_key, api_secret, format='json')\n",
    "\n",
    "\n",
    "# In[455]:\n",
    "\n",
    "\n",
    "json.loads(flickr.photos.licenses.getInfo().decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# In[410]:\n",
    "\n",
    "\n",
    "def get_url(photo_id=\"33510015330\"):\n",
    "    response = flickr.photos.getsizes(photo_id=photo_id)\n",
    "    sizes = json.loads(response.decode('utf-8'))['sizes']['size']\n",
    "    for size in sizes:\n",
    "        if size['label'] == \"Small\":\n",
    "            return size['source']\n",
    "            \n",
    "get_url()\n",
    "\n",
    "\n",
    "# In[433]:\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(\"<img src='https://farm4.staticflickr.com/3803/33510015330_d1fc801d16_m.jpg'>\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# you can check the imagenet classes at https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "\n",
    "\n",
    "# In[469]:\n",
    "\n",
    "\n",
    "def search_ids(search_string=\"python\", per_page=10):\n",
    "    photos_response = flickr.photos.search(text=search_string, per_page=per_page, sort='relevance')\n",
    "    photos = json.loads(photos_response.decode('utf-8'))['photos']['photo']\n",
    "    ids = [photo['id'] for photo in photos]\n",
    "    return ids\n",
    "\n",
    "\n",
    "# In[471]:\n",
    "\n",
    "\n",
    "ids = search_ids(\"ball snake\", per_page=100)\n",
    "urls_ball = [get_url(photo_id=i) for i in ids]\n",
    "img_string = \"\\n\".join([\"<img src='{}'>\".format(url) for url in urls_ball])\n",
    "HTML(img_string)\n",
    "\n",
    "\n",
    "# In[472]:\n",
    "\n",
    "\n",
    "ids = search_ids(\"carpet python\", per_page=100)\n",
    "urls_carpet = [get_url(photo_id=i) for i in ids]\n",
    "img_string = \"\\n\".join([\"<img src='{}'>\".format(url) for url in urls_carpet])\n",
    "HTML(img_string)\n",
    "\n",
    "\n",
    "# In[480]:\n",
    "\n",
    "\n",
    "get_ipython().system('mkdir -p snakes/carpet')\n",
    "get_ipython().system('mkdir snakes/ball')\n",
    "\n",
    "\n",
    "# In[481]:\n",
    "\n",
    "\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "for url in urls_carpet:\n",
    "    urlretrieve(url, os.path.join(\"snakes\", \"carpet\", os.path.basename(url)))\n",
    "\n",
    "\n",
    "# In[482]:\n",
    "\n",
    "\n",
    "for url in urls_ball:\n",
    "    urlretrieve(url, os.path.join(\"snakes\", \"ball\", os.path.basename(url)))\n",
    "\n",
    "\n",
    "# In[511]:\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "images_carpet = [image.load_img(os.path.join(\"snakes\", \"carpet\", os.path.basename(url)), target_size=(224, 224))\n",
    "                 for url in urls_carpet]\n",
    "images_ball = [image.load_img(os.path.join(\"snakes\", \"ball\", os.path.basename(url)), target_size=(224, 224))\n",
    "                 for url in urls_ball]\n",
    "X = np.array([image.img_to_array(img) for img in images_carpet + images_ball])\n",
    "\n",
    "\n",
    "# In[527]:\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "images_carpet = [image.load_img(os.path.join(\"snakes\", \"carpet\", os.path.basename(url)), target_size=(224, 224))\n",
    "                 for url in urls_carpet]\n",
    "images_ball = [image.load_img(os.path.join(\"snakes\", \"ball\", os.path.basename(url)), target_size=(224, 224))\n",
    "                 for url in urls_ball]\n",
    "X = np.array([image.img_to_array(img) for img in images_carpet + images_ball])\n",
    "\n",
    "\n",
    "# In[566]:\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(6, 4, subplot_kw={'xticks': (), 'yticks': ()}, figsize=(5, 8))\n",
    "for img, ax in zip(images_carpet, axes.ravel()):\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "# In[565]:\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(6, 4, subplot_kw={'xticks': (), 'yticks': ()}, figsize=(5, 8))\n",
    "for img, ax in zip(images_ball, axes.ravel()):\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "# In[529]:\n",
    "\n",
    "\n",
    "X.shape\n",
    "\n",
    "\n",
    "# In[530]:\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "X_pre = preprocess_input(X)\n",
    "features = model.predict(X_pre)\n",
    "\n",
    "\n",
    "# In[531]:\n",
    "\n",
    "\n",
    "features.shape\n",
    "\n",
    "\n",
    "# In[532]:\n",
    "\n",
    "\n",
    "features_ = features.reshape(200, -1)\n",
    "\n",
    "\n",
    "# In[537]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = np.zeros(200, dtype='int')\n",
    "y[100:] = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_, y, stratify=y)\n",
    "\n",
    "\n",
    "# In[548]:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr = LogisticRegressionCV().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# In[549]:\n",
    "\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "\n",
    "\n",
    "# In[550]:\n",
    "\n",
    "\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# In[553]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, lr.predict(X_test))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
