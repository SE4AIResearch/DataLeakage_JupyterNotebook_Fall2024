{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import shapefile\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from patsy import dmatrices\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import pickle\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import preprocessing as prp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pylab import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# ### Load data\n",
    "\n",
    "# In[73]:\n",
    "\n",
    "\n",
    "with open('full_data.pkl','r') as f:\n",
    "    df_full = pickle.load(f) # generated in 'citibike' notebook\n",
    "    \n",
    "df_bikeid = pd.read_csv('bikeids.csv') # database sorted chronologically and by bike id\n",
    "\n",
    "\n",
    "# ### Reformat and add columns to bikeid dataset\n",
    "# ####Note: same treatment as was done to df_full in 'citibike' notebook\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# Hour of the day column\n",
    "df_bikeid['starttime_time'] = df_bikeid['starttime'].map(lambda x: time.strptime(x, \"%Y-%m-%d %H:%M:%S\").tm_hour*60 + \n",
    "                                      time.strptime(x, \"%Y-%m-%d %H:%M:%S\").tm_min)\n",
    "\n",
    "# create 'over_45' column (1 if yes, 0 if no) -- for exploration purposes\n",
    "df_bikeid['over_45'] = df_bikeid.tripduration.map(lambda x: 1 if x>2700 else 0)\n",
    "\n",
    "week   = ['Monday', 'Tuesday', 'Wednesday', \n",
    "              'Thursday', 'Friday', 'Saturday','Sunday']\n",
    "\n",
    "# Column for day of the week (0-6)\n",
    "df_bikeid['day_start']=df_bikeid['starttime'].map(lambda x: week[datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").weekday()])\n",
    "\n",
    "# To datetime objects\n",
    "df_bikeid['starttime'] = df_bikeid['starttime'].map(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "df_bikeid['stoptime'] = df_bikeid['stoptime'].map(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# shift data down\n",
    "t = df_bikeid.shift(1)\n",
    "t = t[['stoptime','end_station_id','n2','day_start','bikeid']]\n",
    "t.columns = [x+\"_\" for x in t.columns]\n",
    "\n",
    "# add a shifted-down version of the database to itself, to analyze rebalancing ratio\n",
    "df_bikes = pd.concat((t,df_bikeid),1) \n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "len(df_bikes)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "# Remove overlaps of bikes\n",
    "df_bikes = df_bikes[df_bikes['bikeid_']==df_bikeid['bikeid']]\n",
    "\n",
    "# Create time lapse and same station column\n",
    "df_bikes['time_lapsed'] = df_bikes.apply(lambda row: (row['starttime'] - row['stoptime_']), axis=1)\n",
    "df_bikes['same_station'] = df_bikes.apply(lambda row: True if row['end_station_id_'] == row['start_station_id'] else False, axis=1)\n",
    "\n",
    "# Remove buggy trips (where bike departs before it's been dropped off)\n",
    "bad_bikes = df_bikes[df_bikes['time_lapsed']<0].bikeid.unique()\n",
    "df_good_bikes = df_bikes[-df_bikes['bikeid'].isin(bad_bikes)]\n",
    "\n",
    "\n",
    "# ### Various pickling of database versions and modeling results\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "with open('bikeid_data_good.pkl','wb') as f:\n",
    "    pickle.dump(df_good_bikes,f)\n",
    "    \n",
    "with open('model_results_1.pkl','r') as f:\n",
    "    r_1 = pickle.load(f)\n",
    "    \n",
    "with open('bikeid_data_good.pkl','r') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "# full dataset: df_full\n",
    "\n",
    "# only subscribers have gender and birth year information\n",
    "# subset to include those features, and disregard birth years before 1915\n",
    "df_sub = df_full[(df_full['birth_year']>1915.0) & (df_full['usertype']=='Subscriber') \n",
    "                 & (df_full['gender']!=0)& (df_full['tripduration']<=3600)]  \n",
    "\n",
    "print((len(df_full)))\n",
    "print((len(df_sub)))\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "# convert cumulative trip duration variable into float for future analysis\n",
    "df['cum_amt'] = df['cum_amt'].map(lambda f : float(f)) \n",
    "\n",
    "\n",
    "# ### Split data into features and response variable\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "\n",
    "def assign_train_test(df_any, sub=True):\n",
    "    '''Assign y and X. \n",
    "    Dummify categorical variables (neighborhood, weekday, gender).\n",
    "    Standardize continuous variables.\n",
    "    Returns train and test subsets'''\n",
    "    \n",
    "    # response variable is destination neighborhood\n",
    "    \n",
    "    # features depend on whether analysis on all data or subscriber data\n",
    "    \n",
    "    # Subscriber data, classification (includes information we would only know after the fact)\n",
    "    if sub==True:\n",
    "        y = df_any['n2']\n",
    "        cols_n = ['tripduration','birth_year','starttime_time','docks_y'] # features to Standardize\n",
    "        cols_c = ['n1','gender','day_start'] # categorical features\n",
    "    \n",
    "    # All data (subscribers and customers)\n",
    "    elif sub==False:\n",
    "        y = df_any['n2']\n",
    "        cols_n = ['tripduration','starttime_time','docks_y'] # features to Standardize\n",
    "        cols_c = ['n1','day_start','usertype'] # categorical features. include usertype as feature\n",
    "    \n",
    "    # Bikeid data (to predict rebalancing)\n",
    "    elif sub=='bikes':\n",
    "        y = df_any['same_station']\n",
    "        cols_n = ['cum_amt']\n",
    "        cols_c = ['n2_','day_start_']\n",
    "    \n",
    "    # Subscriber data, real prediction (excludes information we would only know after the fact)\n",
    "    else:\n",
    "        y = df_any['n2']\n",
    "        cols_n = ['birth_year','starttime_time'] # features to Standardize\n",
    "        cols_c = ['gender','day_start','n1']\n",
    "    \n",
    "    X = df_any[cols_n + cols_c]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "    scaler = StandardScaler() # Standardize continuous variables\n",
    "    scaler.fit(X_train[cols_n]) # Fit standardization rule to training data\n",
    "        \n",
    "    # continous variable treatment\n",
    "    X_train_1 = pd.DataFrame(scaler.transform(X_train[cols_n]), columns=cols_n)\n",
    "    X_test_1 = pd.DataFrame(scaler.transform(X_test[cols_n]), columns=cols_n)\n",
    "    \n",
    "    # categorical variables\n",
    "    X_train_2 = pd.get_dummies(X_train[cols_c], columns = cols_c)\n",
    "    X_test_2 = pd.get_dummies(X_test[cols_c], columns = cols_c)\n",
    "    \n",
    "    # bring 'em together\n",
    "    X_train = pd.concat((X_train_1.reset_index(drop=True),X_train_2.reset_index(drop=True)),1)\n",
    "    X_test = pd.concat((X_test_1.reset_index(drop=True),X_test_2.reset_index(drop=True)),1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# In[83]:\n",
    "\n",
    "\n",
    "#generate test/train data\n",
    "#assign 'sub' parameter to arbitrary value to evaluate dataset with the following:\n",
    "#  features: birth_year, starttime_time, gender, day_start, n1 (starting neighborhood)\n",
    "#  target: n2 (destination neighborhood)\n",
    "\n",
    "X_train, X_test, y_train, y_test = assign_train_test(df_sub, sub = \"hey\")\n",
    "\n",
    "\n",
    "# In[84]:\n",
    "\n",
    "\n",
    "def conf_matrix(clf):\n",
    "    print(('_' * 80))\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train) # fit model\n",
    "    train_time = time() - t0\n",
    "    print((\"train time: %0.3fs\" % train_time))\n",
    "    \n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test) # predict \n",
    "    test_time = time() - t0\n",
    "    print((\"test time:  %0.3fs\" % test_time))\n",
    "    \n",
    "    return confusion_matrix(y_test, pred, labels=['True','False'])\n",
    "\n",
    "\n",
    "# In[86]:\n",
    "\n",
    "\n",
    "# Function to fit model, print training time and prediction time, \n",
    "# then print accuracy, classification report and feature importances\n",
    "\n",
    "def benchmark(clf):\n",
    "    print(('_' * 80))\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train) # fit model\n",
    "    train_time = time.time() - t0\n",
    "    print((\"train time: %0.3fs\" % train_time))\n",
    "\n",
    "    t0 = time.time()\n",
    "    pred = clf.predict(X_test) # predict \n",
    "    test_time = time.time() - t0\n",
    "    print((\"test time:  %0.3fs\" % test_time))\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred) # accuracy\n",
    "    print((\"accuracy:   %0.3f\" % score))\n",
    "    \n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test,pred, average = 'weighted')\n",
    "    \n",
    "    try:\n",
    "        feats = clf.feature_importances_\n",
    "        indices = np.argsort(feats)[::-1]\n",
    "        print(\"Feature ranking:\")\n",
    "        for f in range(len(feats)):\n",
    "            print((\"%d. feature %d (%f)\" % (f + 1, indices[f], feats[indices[f]])))\n",
    "    \n",
    "    except:\n",
    "        feats = 0\n",
    "            \n",
    "    clf_descr = str(clf).split('(')[0] # string name of classifier\n",
    "    return clf_descr, score, prec, rec, f1, feats\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "# Calculate confusion matrix for Extra Trees Classifier\n",
    "\n",
    "cm = []\n",
    "clf, name = (ExtraTreesClassifier(n_estimators = 15, class_weight = 'auto', \n",
    "                              n_jobs = -1, random_state = 0), \"Extra Trees\")\n",
    "print(('=' * 80))\n",
    "print(name)\n",
    "cm.append(conf_matrix(clf))\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "\n",
    "# Compute accuracy, other scores and feature importances for certain models\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RandomForestClassifier(n_estimators = 10, n_jobs = -1, class_weight='auto'), \n",
    "         \"Random Forest\"),\n",
    "        (ExtraTreesClassifier(n_estimators = 10, class_weight = 'auto', \n",
    "                              n_jobs = -1, random_state = 0), \"Extra Trees\")):\n",
    "    print(('=' * 80))\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# Example of feature importances\n",
    "\n",
    "sorted(zip(X_test.columns,results[1][5]),key=lambda x: x[1])\n",
    "\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "\n",
    "# More models to run\n",
    "# Perceptron is the same as Stochastic Gradient Descent with certain parameters:\n",
    "# SGDClassifier(loss=”perceptron”, eta0=1, learning_rate=”constant”, penalty=None)\n",
    "\n",
    "results = []\n",
    "\n",
    "for clf, name in (\n",
    "        (DecisionTreeClassifier(class_weight='auto'), \"Decision Tree\"),\n",
    "        (Perceptron(n_jobs = -1, n_iter = 10, class_weight = 'auto'), \"Perceptron\"),\n",
    "        (LogisticRegression(class_weight = 'auto'), \"Logistic\"),\n",
    "        (SGDClassifier(class_weight = 'auto', n_jobs = -1, random_state = 1), \n",
    "         \"Stochastic Gradient Descent\")):\n",
    "    print(('=' * 80))\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "print(('=' * 80))\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', LinearSVC(penalty = 'l1',dual=False, tol=1e-3, class_weight = 'auto')),\n",
    "  ('classification', RandomForestClassifier(n_estimators = 5))\n",
    "])))\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "# Experiment with AdaBoost Classifier\n",
    "# Accuracy scores were higher but computationally expensive\n",
    "\n",
    "clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(class_weight='auto'),\n",
    "    n_estimators=30,\n",
    "    learning_rate=1, random_state = 1)\n",
    "results.append(benchmark(clf))\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "\n",
    "cm_array = results[1][6]\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Actual versus Predicted', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm.squeeze(), interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title,size=15)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(37)\n",
    "    plt.xticks(tick_marks, sorted(df_sub.n1.unique()),rotation=90)\n",
    "    plt.yticks(tick_marks, sorted(df_sub.n1.unique()))\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# In[94]:\n",
    "\n",
    "\n",
    "# Confusion matrix (without normalization)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print('Actual versus Predicted Destination Neighborhoods (without normalization)')\n",
    "print(cm_array)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_confusion_matrix(cm_array)\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "\n",
    "cm_normalized = cm_array.astype('float') / cm_array.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "plt.figure(figsize=(14, 10))\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "\n",
    "# In[168]:\n",
    "\n",
    "\n",
    "# Create rebalance rate in dataframe for neighborhood (\"n2_\") and stations (\"end_station_id_\")\n",
    "\n",
    "import operator\n",
    "\n",
    "t = pd.DataFrame(df.groupby(\"n2_\").same_station.value_counts())\n",
    "d = {}\n",
    "\n",
    "for n in df.n2_.unique():\n",
    "    d[n] = ((t.loc[n].iloc[1]).values/((t.loc[n].iloc[0]).values + (t.loc[n].iloc[1]).values))\n",
    "    \n",
    "sorted_x = sorted(list(d.items()), key=operator.itemgetter(1))\n",
    "sorted_x.reverse()\n",
    "\n",
    "df_stations = pd.DataFrame(sorted_x)\n",
    "df_stations.columns = ['Neighborhood','Removal rate']\n",
    "\n",
    "\n",
    "# In[114]:\n",
    "\n",
    "\n",
    "# Snapshot of bike data on bike 21415\n",
    "\n",
    "bike_21415 = df[df['bikeid']==21415][['bikeid','stoptime_','end_station_id_','n2_',\n",
    "                                'starttime','start_station_id','n1','same_station']]\n",
    "bike_21415\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
