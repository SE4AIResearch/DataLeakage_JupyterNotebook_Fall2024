{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/dataset_challenge_one (6).tsv', sep='\\t')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "df.iloc[:,:10].describe()\n",
    "\n",
    "\n",
    "# * **More variables than instances (rows) - need dimensionality reduction or feature selection to avoid overfitting **\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# Check if class label is biased\n",
    "Counter(df['class'])\n",
    "\n",
    "\n",
    "# ## Data Cleaning\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "print('Pre-Clean: {}'.format(df.shape))\n",
    "df.dropna(inplace=True)\n",
    "print('Post-Clean: {}'.format(df.shape))\n",
    "\n",
    "\n",
    "# * ** Removed two rows (alternatively, can use collaborative filtering to predict missing values if they are crucial part of overall analysis)**\n",
    "\n",
    "# ## Data Exploration and Visualizations\n",
    "\n",
    "# ### Review summary statistics for each variable\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "df_vars = df.drop('class', axis=1)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "summary_stats = df_vars.describe().loc[['min','max','mean','std']]\n",
    "summary_stats = summary_stats.transpose()\n",
    "summary_stats['range'] = summary_stats['max'] - summary_stats['min']\n",
    "summary_stats.head()\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "f, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2, figsize=(20,12))\n",
    "ax1.set_title('Distribution of Variable Min.', size=16)\n",
    "ax2.set_title('Distribution of Variable Max.', size=16)\n",
    "ax3.set_title('Distribution of Variable Means', size=16)\n",
    "ax4.set_title('Distribution of Variable Variances', size=16)\n",
    "sns.distplot(summary_stats['min'], axlabel=False, ax=ax1, )\n",
    "sns.distplot(summary_stats['max'], axlabel=False, ax=ax2)\n",
    "sns.distplot(df_vars[df['class']==0].mean(), bins=20, axlabel=False, label='Class=0', ax=ax3)\n",
    "sns.distplot(df_vars[df['class']==1].mean(), bins=20, axlabel=False, label='Class=1', ax=ax3)\n",
    "ax3.legend()\n",
    "sns.distplot(summary_stats['std'].map(lambda x: x**2), axlabel=False, ax=ax4)\n",
    "\n",
    "\n",
    "# * ** Purpose of these plots: to understand the nature of the variables **\n",
    "# \n",
    "# ** Observations: **\n",
    "# * ** Min. and max. values are lower and upper bounded by -2 and +2 respectively **\n",
    "# * ** Note the negative skew in the variable means **\n",
    "# * ** Variable means are more dispersed given class = 1 - higher entropy when class = 1 **\n",
    "# * ** Variable variances are centered between 0.05 to 0.1 **\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "f, ([ax5, ax6]) = plt.subplots(1, 2, figsize=(20,6))\n",
    "ax5.set_title('Distribution of All Variables (Using Kernel Density Estimate, Alpha=0.1)', size=16)\n",
    "ax6.set_title('Distribution of All Variables (Using Kernel Density Estimate, Alpha=0.01)', size=16)\n",
    "for column in df_vars.columns:\n",
    "    sns.distplot(df_vars[column], color='gray', hist=False, axlabel=False, ax=ax5, kde_kws={'alpha':0.1})\n",
    "    sns.distplot(df_vars[column], color='gray', hist=False, axlabel=False, ax=ax6, kde_kws={'alpha':0.01})\n",
    "\n",
    "\n",
    "# * **A sizable number of variables have high excess kurtosis**\n",
    "\n",
    "# ### Correlation between class label and variables\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "corr_versus_class = df.ix[:, df.columns != 'class'].corrwith(df['class'])\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.title('Correlation between Variable vs. Class Label Distribution', size=20)\n",
    "plt.xlabel('Variable and Class Label Correlations', size=16)\n",
    "sns.distplot(corr_versus_class)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "np.percentile(corr_versus_class, 50)\n",
    "\n",
    "\n",
    "# **Observations:**\n",
    "# * ** Max. and min. correlations near +/- 0.4 **\n",
    "# * ** Roughly 50% of variables have negative correlation with class label **\n",
    "\n",
    "# ** Sort variables by correlation with class label: **\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "labels = df['class']\n",
    "df_vars = df.drop('class', axis=1)\n",
    "df_vars = df_vars.iloc[:, corr_versus_class.argsort()[::-1]]\n",
    "\n",
    "\n",
    "# * ** Dataframe variables (columns) are now sorted by correlation with class labels **\n",
    "\n",
    "# ### Correlation Matrix\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Correlation Matrix Heatmap between all Variables', size=16)\n",
    "sns.heatmap(df_vars.corr(), xticklabels=False, yticklabels=False)\n",
    "\n",
    "\n",
    "# * ** Of course it's not possible to make variable-specific interpretation with a heatmap like this - but this is useful to get a very high-level gauge on the correlation matrix **\n",
    "# \n",
    "# ** A few Oberservations:**\n",
    "# * Overall map is more red than blue (e.g. if the variables are financial time-series and economic indicators, most of the variables will have a positive correlation and blue-ish column vectors indicate risk-off indicators (gold, safe-haven currencies, VIX, inverse-index ETFs, etc.)\n",
    "# * To predict class label, will be useful to use variables with high and low correlations - however, must check for collinearity due to large degree of collinearity (use PCA in later steps)\n",
    "\n",
    "# ### Anomalous Distributions\n",
    "# \n",
    "# ** Test to see if distribution is close to normal use quantile-quantile plots**\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "from scipy.stats import probplot\n",
    "\n",
    "corr_coeffs = []\n",
    "for column in df_vars.columns:\n",
    "    _, (slope, intercept, r) = probplot(df_vars[column], dist=\"norm\")\n",
    "    corr_coeffs.append(r)\n",
    "\n",
    "\n",
    "# ** *Example of QQ-plot* **\n",
    "# \n",
    "# * ** If distribution is perfectly normal, points should fall on line **\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "res = probplot(df_vars.iloc[:, np.argsort(corr_coeffs)[0]], dist=\"norm\", plot=plt)\n",
    "plt.title(\"Quantile-Quantile Plot\", size=16)\n",
    "plt.xlabel(\"Theoretical Quantile\")\n",
    "plt.ylabel(\"Sample Quantile\")\n",
    "\n",
    "\n",
    "# ** Track correlation coefficient of each QQ-plot (Quantile-Quantile Plot)** \n",
    "# * Series with lowest values may not be normally distributed\n",
    "# \n",
    "# ** Plotting top 5 variables with lowest correlation coefficients**:\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Distributions of Lowest Correlated QQ-plots', size=18)\n",
    "anomaly_indices = np.argsort(corr_coeffs)[:20]\n",
    "for i in anomaly_indices:\n",
    "    sns.distplot(df_vars.iloc[:, i], hist=False)\n",
    "\n",
    "\n",
    "# **Observation: Variables appear to be approx. normally distributed with varying degrees of variance, skewness and kurtosis**\n",
    "\n",
    "# # 2. PCA Analysis\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(df_vars.values)\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "df_clusters = pd.DataFrame(reduced_data, columns=['PC1','PC2'])\n",
    "df_clusters['class'] = labels\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "sns.lmplot(x='PC1', y='PC2', hue='class', data=df_clusters, size=8, fit_reg=False)\n",
    "plt.title('PCA (Components=2)', size=16)\n",
    "\n",
    "\n",
    "# # 3A. Variable Relationship with Class Column\n",
    "\n",
    "# ** For all variables, calculate correlation coefficients with class label **\n",
    "# \n",
    "# * ** *Since the class label is binary, the Pearson coefficient is equal to the point biserial correlation coefficient - this statistic can be used to compute the relationship with class column* **\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "# corr_versus_class already calculated from previous step\n",
    "# this is the correlations between each variable and class labels\n",
    "corr_versus_class.max(), corr_versus_class.min()\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "sig_var_name = corr_versus_class.argmax()\n",
    "'Variable with most significant statistic: {}'.format(sig_var_name)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "_df = pd.DataFrame(df[corr_versus_class.argmax()])\n",
    "_df['class'] = labels\n",
    "_df.columns = ['variable','class']\n",
    "_df.dropna(inplace=True)\n",
    "_df.head()\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (16, 5)})\n",
    "g = sns.distplot(_df[_df['class'] == 0].variable, label='class=0')\n",
    "sns.distplot(_df[_df['class'] == 1].variable, label='class=1', ax=g, axlabel=sig_var_name)\n",
    "g.legend().set_visible(True)\n",
    "plt.title('Distribution Plots for Variable 1497 (by Class)', size=16)\n",
    "\n",
    "\n",
    "# * **Factor plots are a good choice when comparing categorical variable with continues variable because it gives a probability interpretation to the exact maximum likelihood range for each class label (assuming equal priors)**\n",
    "#     * e.g. in this example, when variable is < -0.1 (approx), higher likelihood for class 0\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (6, 6)})\n",
    "sns.boxplot(x=\"class\", y='variable', data=_df)\n",
    "plt.title('Boxplots for Variable 1497 (by Class)', size=12)\n",
    "\n",
    "\n",
    "# ** Using boxplot as factor plots, it's clear that Variable 1497 is a useful predictor for the class label. **\n",
    "# \n",
    "# ** *Any useful summary statistic should have a noticeable mean difference between the two classes and the joint variance should be low* **\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "sns.swarmplot(x=\"class\", y='variable', data=_df)\n",
    "plt.title('Swarmplots for Variable 1497 (by Class)', size=12)\n",
    "\n",
    "\n",
    "# # 3B. Variable Relationship with PC1\n",
    "\n",
    "# ** Most significant statistic used is the R-squared **\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "rsquared_with_pca = df_vars.corrwith(df_clusters.PC1).map(lambda x: x**2)\n",
    "'Highest R-Squared: {} - {} and PC1'.format(rsquared_with_pca.max(), np.argmax(rsquared_with_pca))\n",
    "\n",
    "\n",
    "# ** Choose variable with highest R-squared with PC1 **\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "sig_var_name = rsquared_with_pca.argmax()\n",
    "'Variable with most significant statistic: {}'.format(sig_var_name)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "_df = None\n",
    "_df = pd.DataFrame(df[sig_var_name])\n",
    "_df['pc1'] = df_clusters.PC1\n",
    "_df['class'] = labels\n",
    "\n",
    "_df.columns = ['variable','pc1', 'class']\n",
    "_df.dropna(inplace=True)\n",
    "_df.head()\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "sns.lmplot(x=\"pc1\", y='variable', data=_df, )\n",
    "plt.title('Scatterplot - Variable 578 vs. PC1')\n",
    "sns.lmplot(x=\"pc1\", y='variable', hue='class', data=_df, fit_reg=False)\n",
    "plt.title('Scatterplot - Variable 578 vs. PC1 (by Class)')\n",
    "\n",
    "\n",
    "# ** Residual Plot between variable and PC1:** \n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (10, 4)})\n",
    "sns.residplot('pc1', 'variable', data=_df)\n",
    "plt.title('Residual Plot for Variable 578 vs. PC1', size=16)\n",
    "\n",
    "\n",
    "# Residual Plot shows that the variance is not constant, variance tends to increase with PC1\n",
    "# \n",
    "# *This would be a concern if we are using PC1 to predict the value of the variable*\n",
    "\n",
    "# # 4. Classifier Model\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_vars.values, labels.values, test_size=0.3, random_state=5)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "verbose = False\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "## Model - Generic Model (All Variables) + PCA Feature Extraction + Elastic Net\n",
    "\n",
    "kfold = StratifiedKFold(y=y_train, n_folds=4, shuffle=True, random_state=5)\n",
    "params_range = {'loss': ['hinge'], # hinge loss equivalent to SVM\n",
    "                'alpha': [0.001, 0.01, 0.1, 1],\n",
    "                'l1_ratio': list(np.arange(0, 1.2, 0.2)),\n",
    "                'n_components': list(range(2, 30, 2))}\n",
    "results = []\n",
    "\n",
    "## Hyper-tuning # of components via Grid Search\n",
    "for loss, alpha, l1_ratio, n_component in product(params_range['loss'], params_range['alpha'], \n",
    "                                                  params_range['l1_ratio'], params_range['n_components']):\n",
    "    \n",
    "    # PCA - Feature Extraction\n",
    "    pca = PCA(n_components=n_component)\n",
    "    reduced_data = pca.fit_transform(X_train)\n",
    "    \n",
    "    # Classifier\n",
    "    clf = SGDClassifier(loss=loss, penalty='elasticnet', alpha=alpha, l1_ratio=l1_ratio, n_iter=1000)\n",
    "    \n",
    "    # Iterate k-folds\n",
    "    scores_accuracy, scores_f1 = [], []\n",
    "    for k, (train_index, cv_index) in enumerate(kfold):\n",
    "        \n",
    "        # Train model\n",
    "        clf.fit(reduced_data[train_index], y_train[train_index])\n",
    "        \n",
    "        # Predict\n",
    "        pred = clf.predict(reduced_data[cv_index])\n",
    "        \n",
    "        score_accuracy = round(accuracy_score(y_train[cv_index], pred), 3)\n",
    "        score_f1 = round(f1_score(y_train[cv_index], pred), 3)\n",
    "        \n",
    "        scores_accuracy.append(score_accuracy)\n",
    "        scores_f1.append(score_f1)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Fold: {}, Accuracy: {}, F1-Score: {}, Alpha: {}, L1-ratio: {}, Loss: {}, PCA Components: {}'.format(\n",
    "                k, score_accuracy, score_f1, alpha, l1_ratio, loss, n_component))\n",
    "    \n",
    "    # Track Scores\n",
    "    results.append((round(np.mean(scores_accuracy), 3), round(np.mean(scores_f1), 3), \n",
    "                    alpha, l1_ratio, loss, n_component))\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "print('Best Accuracy: {}'.format(max(results, key=lambda x: x[0])))\n",
    "print('Best F1-Score: {}'.format(max(results, key=lambda x: x[1])))\n",
    "\n",
    "\n",
    "# ### *Use model with best F1-score*\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "# Feature Extraction with all training data\n",
    "pca = PCA(n_components=18)\n",
    "train_reduced_data = pca.fit_transform(X_train)\n",
    "\n",
    "# Training model with all training data\n",
    "clf = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=0.001, l1_ratio=0.6, n_iter=10000)\n",
    "clf.fit(train_reduced_data, y_train)\n",
    "\n",
    "# Transform test data to new subspace\n",
    "test_reduced_data = pca.transform(X_test)\n",
    "pred = clf.predict(test_reduced_data)\n",
    "\n",
    "# Test model\n",
    "score_accuracy = round(accuracy_score(y_test, pred), 3)\n",
    "score_f1 = round(f1_score(y_test, pred), 3)\n",
    "score_precision = round(precision_score(y_test, pred), 3)\n",
    "score_recall = round(recall_score(y_test, pred), 3)\n",
    "\n",
    "score_accuracy, score_f1, score_precision, score_recall\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
